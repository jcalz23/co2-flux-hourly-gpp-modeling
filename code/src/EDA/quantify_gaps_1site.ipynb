{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation and Gap-Filling Logic (Dev)\n",
    "Goal: Quantify gaps for each site to understand extent of the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required modules quietly\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "import math\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.pandas as pd\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load locale custome modules\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../tools\"))\n",
    "\n",
    "from CloudIO.AzStorageClient import AzStorageClient\n",
    "from data_pipeline_lib import *\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n",
    "\n",
    "for p in required_packages: \n",
    "  try:\n",
    "      __import__(p)\n",
    "  except ImportError:\n",
    "      %pip install {p} --quiet\n",
    "\n",
    "MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n",
    "root_dir =  MY_HOME_ABS_PATH\n",
    "tmp_dir =  root_dir + os.sep + '.tmp'\n",
    "raw_data_dir = tmp_dir\n",
    "data_dir = root_dir + os.sep + 'data'\n",
    "cred_dir = root_dir + os.sep + '.cred'\n",
    "az_cred_file = cred_dir + os.sep + 'azblobcred.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare One Site Dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variables of the data pipelines\n",
    "included_features = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA',\n",
    "                     'datetime', 'year', 'month', 'day', 'hour', 'date',\n",
    "                     'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', \n",
    "                     'IGBP', 'koppen']\n",
    "target_variable_qc = 'NEE_VUT_REF_QC'\n",
    "target_variable = 'GPP_NT_VUT_REF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick site, create dummy df\n",
    "site = 'CN-HaM' # <--- reduced to one site by John\n",
    "\n",
    "# Load site metadata\n",
    "included_site_features = ['site_id', 'filename', 'elevation', 'lat', 'long',\n",
    "                          'koppen_sub', 'koppen_main', 'koppen_name',\n",
    "                          'c3c4', 'c4_percent']\n",
    "                          \n",
    "site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'\n",
    "site_metadata_df = pd.read_csv(site_metadata_filename, usecols = included_site_features)\n",
    "\n",
    "# only focus on target sites\n",
    "site_metadata_df = site_metadata_df.loc[site_metadata_df['site_id'].isin([site])]\n",
    "print(f\"size:{site_metadata_df.shape}\")\n",
    "site_metadata_df.reset_index(inplace=True, drop=True)\n",
    "site_metadata_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Site w/ cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load site data\n",
    "def data_cleanup(data_dir, site_id_file_df, target, target_qc, features):\n",
    "  data_df = None\n",
    "  # qc_flag_dtype = CategoricalDtype([0, 1, 2, 3], ordered=True)\n",
    "  qc_flags_features = [s for s in features if \"_QC\" in s]\n",
    "\n",
    "  # Iterate through each site:\n",
    "  for i, r in site_id_file_df.iterrows():        \n",
    "    if not r.filename or type(r.filename) != type(\"\"):\n",
    "      print(f'\\nERROR: {r.site_id} is mssing hourly data.')\n",
    "      continue\n",
    "\n",
    "    # Get only `features` from file\n",
    "    local_filename = data_dir + os.sep + r.filename\n",
    "    site_df = pd.read_csv(local_filename, usecols = [target, target_qc] + features)\n",
    "    site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n",
    "    site_df['date'] = pd.to_datetime(site_df['date'])\n",
    "    site_df['minute'] = site_df['datetime'].dt.minute\n",
    "    if len(qc_flags_features) != 0:\n",
    "      site_df[qc_flags_features] = site_df[qc_flags_features].astype('int')\n",
    "    site_df['site_id'] = r.site_id\n",
    "\n",
    "    # Remove zero or negative SW\n",
    "    #site_df.drop(site_df[site_df['SW_IN_ERA'] <= 0].index, inplace = True) # <---------------- REMOVED BY JOHN, NEED TO DISCUSS\n",
    "    # challenge: For gap-filling a completely blank day.. how do we know where to begin and end the filled timesteps?\n",
    "    # Pt1: If other research doesn't do this, we shouldnn't either in order to compare metrics\n",
    "    # Pt2: If we kept these in, we can always analyze errors per hour\n",
    "    # Pt3: Tradeoff is that we are feeding less meaningfull features to model\n",
    "\n",
    "    # Drop rows with NAs for Target Variable\n",
    "    site_df.dropna(subset=[target], axis=0, inplace=True)\n",
    "\n",
    "    # Drop rows with bad NEE_VUT_REF_QC (aka bad GPP records)\n",
    "    site_df.drop(site_df[site_df[target_qc] == 3].index, inplace = True)\n",
    "    site_df.drop([target_qc], axis=1, inplace=True)\n",
    "\n",
    "    # Drop rows with any NA\n",
    "    #site_df.dropna(axis=0, inplace=True) # <---------------- REMOVED BY JOHN\n",
    "\n",
    "    # Move from HH to H level <---------------- ADDED BY JOHN\n",
    "    site_df = site_df.loc[site_df['minute']==0, ].copy()\n",
    "\n",
    "    print(f\"{r.site_id}: {site_df.shape}\")\n",
    "    if type(data_df) == type(None):\n",
    "      data_df = site_df\n",
    "    else:\n",
    "      data_df = pd.concat([data_df, site_df])\n",
    "          \n",
    "  return data_df\n",
    "\n",
    "# Initial data clean and feature selections from raw data\n",
    "data_df = data_cleanup(raw_data_dir, site_metadata_df,\n",
    "                  target_variable, target_variable_qc,\n",
    "                  included_features)\n",
    "print(f\"Data size after cleanup: {data_df.shape}\")\n",
    "\n",
    "# # Merge with site metadata\n",
    "# data_df = merge_site_metadata(data_df, site_metadata_df.drop(['filename', 'koppen_main', 'koppen_name'], axis=1))\n",
    "# print(f\"Data size after after merged with site metadata: {data_df.shape}\")\n",
    "\n",
    "# Drop rows with NA\n",
    "# check_and_drop_na(data_df) <---------------- REMOVED BY JOHN\n",
    "#print(f\"Data size after after final drop: {data_df.shape}\")\n",
    "\n",
    "#reorder columns\n",
    "features = data_df.columns.to_list()\n",
    "features.remove(target_variable)\n",
    "data_df = data_df[([target_variable] + features)]\n",
    "\n",
    "data_df.reset_index(inplace=True, drop=True)  #<---------------- ADDED BY JOHN\n",
    "\n",
    "display(data_df.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the datetime column as the index\n",
    "data_df_hr = data_df.copy() # <---- do we need?\n",
    "data_df_hr = data_df.set_index('datetime')\n",
    "\n",
    "# Create a new DataFrame with hourly frequency\n",
    "data_df_imp_gf = data_df_hr.resample('H').asfreq()\n",
    "#data_df_imp_gf_sub = data_df_imp_gf[[x for x in data_df_imp.columns if x not in ignore_cols]].copy()\n",
    "\n",
    "# Find missing records\n",
    "missing_df = data_df_imp_gf[data_df_imp_gf.isnull().all(axis=1)]\n",
    "print(f\"Hours missing: Count = {len(missing_df)}, % = {100*len(missing_df)/len(data_df_imp_gf):.1f}\")\n",
    "\n",
    "# Collect metrics - hourly\n",
    "count_hrs = len(missing_df)\n",
    "pct_hrs = round(count_hrs/len(data_df_imp_gf), 3)\n",
    "\n",
    "# Determine missing streaks\n",
    "streaks = {}\n",
    "current_streak = 0\n",
    "\n",
    "for i, row in missing_df.iterrows():\n",
    "    current_streak += 1\n",
    "    next_index = i + pd.Timedelta(hours=1)\n",
    "    if next_index not in missing_df.index:\n",
    "        streaks[i] = current_streak\n",
    "        current_streak = 0\n",
    "\n",
    "# Print the resulting dictionary of missing streaks\n",
    "display(streaks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = data_df_hr.resample('D').asfreq()\n",
    "missing_days = daily_df[daily_df.isnull().all(axis=1)]\n",
    "\n",
    "# Determine missing day streaks\n",
    "streaks_day = {}\n",
    "current_streak = 0\n",
    "\n",
    "for i, row in missing_days.iterrows():\n",
    "    current_streak += 1\n",
    "    next_index = i + pd.Timedelta(days=1)\n",
    "    if next_index not in missing_days.index:\n",
    "        streaks_day[i] = current_streak\n",
    "        current_streak = 0\n",
    "\n",
    "# Print the resulting dictionary of missing day streaks\n",
    "display(streaks_day)\n",
    "\n",
    "# Collect metrics\n",
    "total_days_missing = len(missing_days)\n",
    "count_all_missing_streaks = len(streaks_day)\n",
    "big_streak = 5\n",
    "count_big_missing_streaks = len([x for x in list(streaks_day.values()) if x > big_streak])\n",
    "print(total_days_missing)\n",
    "print(count_all_missing_streaks)\n",
    "print(count_big_missing_streaks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"site_id\", \"total_hours\",  \"count_hrs_missing\", \"pct_hrs_missing\", \"total_full_days_missing\", \n",
    "\"count_all_missing_streaks\", \"count_big_missing_streaks\", \"streaks_hr_dict\", \"streaks_day_dict\"]\n",
    "site_missing_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "site_missing_df.loc[len(site_missing_df)] = \\\n",
    "    [site, total_hours, count_hrs_missing, pct_hrs_missing, total_days_missing, count_all_missing_streaks,\n",
    "     count_big_missing_streaks, streaks_hr, streaks_day]\n",
    "\n",
    "site_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-rYFmMrY7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
