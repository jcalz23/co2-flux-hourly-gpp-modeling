{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["e4IubJbxywkb"],"authorship_tag":"ABX9TyMqnIYbkgcQ+I2B/TZZDwZ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook Setup"],"metadata":{"id":"XaXGMTqJr71d"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"cwL0vFi5qtvh","executionInfo":{"status":"ok","timestamp":1676257131482,"user_tz":480,"elapsed":282,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  IN_COLLAB = True\n","else:\n","  IN_COLLAB = False\n","\n","#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n","MY_HOME_ABS_PATH = \"/content/drive/MyDrive/W210/co2-flux-hourly-gpp-modeling\""]},{"cell_type":"code","source":["if IN_COLLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pep1ddZWr6DS","executionInfo":{"status":"ok","timestamp":1676257132665,"user_tz":480,"elapsed":926,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}},"outputId":"0cc61b18-413c-4e00-fee0-7071ec4d434c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Import Modules"],"metadata":{"id":"y2bHb6TEy3_D"}},{"cell_type":"code","source":["# install required modules quietly\n","required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n","\n","for p in required_packages: \n","  try:\n","      __import__(p)\n","  except ImportError:\n","      %pip install {p} --quiet"],"metadata":{"id":"6M8HuLVTr_yv","executionInfo":{"status":"ok","timestamp":1676257136892,"user_tz":480,"elapsed":4229,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","import math\n","import json\n","\n","from pyspark.sql.functions import col\n","from pyspark.sql import functions as F\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","import pyspark.pandas as pd\n","from calendar import monthrange\n","from datetime import datetime\n","from io import BytesIO\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load locale custome modules\n","import sys\n","if IN_COLLAB:\n","  os.chdir(MY_HOME_ABS_PATH)\n","  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n","else:\n","  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n","\n","from CloudIO.AzStorageClient import AzStorageClient\n","from data_pipeline_lib import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)"],"metadata":{"id":"1iRimJGJsAh7","executionInfo":{"status":"ok","timestamp":1676257138127,"user_tz":480,"elapsed":1240,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Initialize Spark Session"],"metadata":{"id":"e4IubJbxywkb"}},{"cell_type":"code","source":["# Import SparkSession\n","from pyspark.sql import SparkSession\n","# Create a Spark Session\n","spark = SparkSession.builder.master(\"local[*]\").config(\n","    \"spark.jars.packages\", \n","    \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.6\"\n","    ).getOrCreate()\n","# Check Spark Session Information\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"hQqYM94CsFHy","executionInfo":{"status":"ok","timestamp":1676257150661,"user_tz":480,"elapsed":12536,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}},"outputId":"8a25a756-5ebf-4d6c-f65b-1ac4278fb7e9"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f094641aa90>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://9f3bfc17928f:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## Define Local File System Constants"],"metadata":{"id":"QEOCHI64ysC2"}},{"cell_type":"code","source":["root_dir =  MY_HOME_ABS_PATH\n","tmp_dir =  root_dir + os.sep + '.tmp'\n","raw_data_dir = tmp_dir\n","data_dir = root_dir + os.sep + 'data'\n","cred_dir = root_dir + os.sep + '.cred'\n","az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n","\n","if IN_COLLAB:\n","  raw_data_dir = \"/content/drive/MyDrive/CO2_flux_gpp_modeling/DS_capstone_23Spring_CO2/Data/half_hourly_data\"\n","\n","site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'"],"metadata":{"id":"H-AwqUa5sHYq","executionInfo":{"status":"ok","timestamp":1676257150661,"user_tz":480,"elapsed":6,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Load Train and Test dataset from Azure Storage Blob"],"metadata":{"id":"ES9TafVQyVG7"}},{"cell_type":"code","source":["# Define target dataset\n","container = \"baseline-data\"\n","ext = \"parquet\"\n","ver = \"0\"\n","blob_name_base = f\"baseline_all_v_{ver}\"\n","train_blob_name_base = f\"baseline-train-v-{ver}\"\n","test_blob_name_base = f\"baseline-test-v-{ver}\""],"metadata":{"id":"pZcOmGLRzAd9","executionInfo":{"status":"ok","timestamp":1676257150662,"user_tz":480,"elapsed":6,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Initialized Azure Storage Client\n","azStorageClient = AzStorageClient(az_cred_file)\n","sessionkeys = azStorageClient.getSparkSessionKeys()\n","spark.conf.set(sessionkeys[0],sessionkeys[1])"],"metadata":{"id":"UOZGHip0yfGP","executionInfo":{"status":"ok","timestamp":1676257150662,"user_tz":480,"elapsed":5,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Load train dataset\n","train_blob_path = f\"wasbs://{container}@{sessionkeys[2]}.blob.core.windows.net/{train_blob_name_base}\"\n","print(f\"Loading train dataset from {train_blob_path}...\")\n","train_df = spark.read.parquet(train_blob_path)\n","\n","print(f\"Data loaded: {train_df.count()} rows x {len(train_df.columns)} columns.\")\n","print(\"Train data peak:\")\n","train_df.show(5, False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO93FnBUsLdD","outputId":"c2ba5bac-64f6-498c-d4dd-cadeb599b5fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading train dataset from wasbs://baseline-data@mids23spring.blob.core.windows.net/baseline-train-v-0...\n","Data loaded: 1485926 rows x 32 columns.\n","Train data peak:\n"]}]},{"cell_type":"code","source":["# Load test dataset\n","test_blob_path = f\"wasbs://{container}@{sessionkeys[2]}.blob.core.windows.net/{test_blob_name_base}\"\n","print(f\"Loading test dataset from {test_blob_path}...\")\n","test_df = spark.read.parquet(test_blob_path)\n","\n","print(f\"Data loaded: {test_df.count()} rows x {len(test_df.columns)} columns.\")\n","print(\"Test data peak:\")\n","test_df.show(5, False)"],"metadata":{"id":"R7FApytav_3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hr_train_df = train_df.filter(col('minute') == 0)\n","hr_test_df = test_df.filter(col('minute') == 0)"],"metadata":{"id":"q2Lz2LEzIWmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Randome Forest Regressor (Default)"],"metadata":{"id":"j_Ko-f2TsAYb"}},{"cell_type":"code","source":["# Define target variable\n","target_variable = 'GPP_NT_VUT_REF'\n","\n","# Train Model\n","rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_variable,\n","                           seed = 42)\n","model = rf.fit(train_df)"],"metadata":{"id":"1ukUqn37wM03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the Model\n","predictions = model.transform(test_df)\n","\n","# Compute Evaluation Metrics\n","# RMSE\n","evaluator = RegressionEvaluator(labelCol=target_variable, predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = evaluator.evaluate(predictions)\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.5}\")\n","\n","# NSE\n","mean_gpp = predictions.agg(F.avg(target_variable)).first()[0]\n","nse_formula = 1 - (F.sum((predictions[target_variable] - predictions.prediction)**2) / F.sum((predictions[target_variable] - mean_gpp)**2))\n","nse = predictions.agg(nse_formula).first()[0]\n","print(f\"Nash-Sutcliffe Efficiency (NSE): {nse:.5}\")"],"metadata":{"id":"gFd_4TKCzyN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hr_predictions = model.transform(hr_test_df)\n","\n","# RMSE\n","evaluator = RegressionEvaluator(labelCol=target_variable, predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = evaluator.evaluate(hr_predictions)\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.5}\")\n","\n","# NSE\n","mean_gpp = hr_predictions.agg(F.avg(target_variable)).first()[0]\n","nse_formula = 1 - (F.sum((hr_predictions[target_variable] - hr_predictions.prediction)**2) / F.sum((hr_predictions[target_variable] - mean_gpp)**2))\n","nse = hr_predictions.agg(nse_formula).first()[0]\n","print(f\"Nash-Sutcliffe Efficiency (NSE): {nse:.5}\")"],"metadata":{"id":"lA1ngyMZ1sY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Evaulation Visulation"],"metadata":{"id":"Z3C5cAE75C4S"}},{"cell_type":"code","source":["predictions.show(5, False)"],"metadata":{"id":"qySU10325Btj","executionInfo":{"status":"error","timestamp":1676257083444,"user_tz":480,"elapsed":1684,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d11f6f5a-bed1-49de-a5b6-4373c2b0755b"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-66-11498450e8c7>\", line 1, in <module>\n","    predictions.show(5, False)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\", line 615, in show\n","    print(self._jdf.showString(n, int_truncate, vertical))\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1321, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\", line 190, in deco\n","    return f(*a, **kw)\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o1603.showString.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 184.0 failed 1 times, most recent failure: Lost task 0.0 in stage 184.0 (TID 259) (9f3bfc17928f executor driver): java.io.FileNotFoundException: \n","wasbs://baseline-data@mids23spring.blob.core.windows.net/baseline-test-v-0/part-00000-8ea27f72-b3e3-469d-8091-626ff01d5c9b-c000.snappy.parquet: No such file or directory.\n","\n","It is possible the underlying files have been updated. You can explicitly invalidate\n","the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n","recreating the Dataset/DataFrame involved.\n","       \n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.io.FileNotFoundException: \n","wasbs://baseline-data@mids23spring.blob.core.windows.net/baseline-test-v-0/part-00000-8ea27f72-b3e3-469d-8091-626ff01d5c9b-c000.snappy.parquet: No such file or directory.\n","\n","It is possible the underlying files have been updated. You can explicitly invalidate\n","the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n","recreating the Dataset/DataFrame involved.\n","       \n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.8/inspect.py\", line 737, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.8/inspect.py\", line 721, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.8/posixpath.py\", line 379, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","source":[],"metadata":{"id":"afCrQ9Z-ntDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment: Use only hourly data"],"metadata":{"id":"NjGei-odmU7c"}},{"cell_type":"code","source":["# Define target variable\n","target_variable = 'GPP_NT_VUT_REF'\n","\n","# Train Model\n","rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_variable,\n","                           seed = 42)\n","model = rf.fit(hr_train_df)\n","\n","# Evaluate the Model\n","hr_predict = model.transform(hr_test_df)\n","\n","# Compute Evaluation Metrics\n","# RMSE\n","evaluator = RegressionEvaluator(labelCol=target_variable, predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = evaluator.evaluate(hr_predict)\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.5}\")\n","\n","# NSE\n","mean_gpp = hr_predict.agg(F.avg(target_variable)).first()[0]\n","nse_formula = 1 - (F.sum((hr_predict[target_variable] - hr_predict.prediction)**2) / F.sum((hr_predict[target_variable] - mean_gpp)**2))\n","nse = hr_predict.agg(nse_formula).first()[0]\n","print(f\"Nash-Sutcliffe Efficiency (NSE): {nse:.5}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkkwxh9mmVUU","executionInfo":{"status":"ok","timestamp":1676255482489,"user_tz":480,"elapsed":149891,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"}},"outputId":"d0705c60-9f29-41dc-d008-160ed4ee0979"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Root Mean Squared Error (RMSE): 5.0951\n","Nash-Sutcliffe Efficiency (NSE): 0.57932\n"]}]}]}