{"cells":[{"cell_type":"markdown","metadata":{"id":"4-mBiDcbrZ1f"},"source":["# Notebook Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":883,"status":"ok","timestamp":1676804752494,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"0vOPrR8CrO1r","outputId":"97955b62-22e1-4653-cdb5-0b475838d897"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  IN_COLLAB = True\n","else:\n","  IN_COLLAB = False\n","\n","#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n","MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n","\n","if IN_COLLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"nuowBABbrhti"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1376,"status":"ok","timestamp":1676804760778,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"noZwsViCrnAS"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install required modules quietly\n","required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n","\n","for p in required_packages: \n","  try:\n","      __import__(p)\n","  except ImportError:\n","      %pip install {p} --quiet\n","\n","import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","os.chdir(MY_HOME_ABS_PATH) # <------------------ ADDED\n","import math\n","import json\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from pyspark.sql.functions import col\n","import pyspark.pandas as pd\n","from calendar import monthrange\n","from datetime import datetime\n","from io import BytesIO\n","from tqdm import tqdm\n","from sklearn.impute import KNNImputer\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load locale custome modules\n","import sys\n","if IN_COLLAB:\n","  os.chdir(MY_HOME_ABS_PATH)\n","  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n","else:\n","  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n","\n","from CloudIO.AzStorageClient import AzStorageClient\n","from data_pipeline_lib import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)\n","\n","\n","# Define paths\n","root_dir =  MY_HOME_ABS_PATH\n","tmp_dir =  root_dir + os.sep + '.tmp'\n","raw_data_dir = root_dir + os.sep + 'data'\n","data_dir = root_dir + os.sep + 'data/datasets'\n","cred_dir = root_dir + os.sep + '.cred'\n","az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n","if IN_COLLAB:\n","  raw_data_dir = \"/content/drive/MyDrive/CO2_flux_gpp_modeling/DS_capstone_23Spring_CO2/Data\""]},{"cell_type":"markdown","metadata":{"id":"2IwvFVLFrtkM"},"source":["# Constant Definitions"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1676804775178,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"KXgevZgWrxiV"},"outputs":[],"source":["# Select monthly features to use\n","included_features= ['SITE_ID', 'year', 'month', 'TIMESTAMP',\n","                   'ESACCI-sm',    # ESACCI Soil Moisture (%)\n","                   'Percent_Snow', # Percentage of snow cover (%)\n","                   'NDWI',      # Normalized Different Water Index (NDWI)\n","                   'PET',       # Potential ET (m)\n","                   'MODIS_PFT', # Plant Function Type\n","                   'MODIS_LC',  # MODIS Land Cover\n","                   'Ts',        # Skin temperature (K) ??\n","                   'LST_Day',   # Daytime land surface temperature (K)\n","                   'LST_Night', # Nightime land surface temperature (K)\n","                   'Lai',       # Leaf Area Index (LAI)\n","                   'Fpar',      # Fraction of photosynthetically active radiation (fPAR)\n","                   'CSIF-SIFdaily', # All-sky daily average SIF\n","                   'BESS-PAR',      # Photosynthetic Active Radiation (PAR) (W/m^2)\n","                   'BESS-PARdiff',  # Diffuse PAR (W/m^2)\n","                   'BESS-RSDN'      # Shortwave downwelling radiation (W/m^2)\n","                   ]\n","\n","# Define in and out files for monthly data\n","monthly_data_input_fname = data_dir + os.sep + 'data_monthly_v1_0.csv'\n","monthly_data_output_fname = raw_data_dir + os.sep + \"monthly-interpolated-v3.csv\"\n","\n","# Define methods for filling NA (interpolate or -1) and gap-fill (fill or leave be)\n","impute = True\n","impute_method = 'interpolate' # other options are 'interpolate', 'knn', 'constant' or None\n","resample_monthly = True\n","knn_imp_cols = ['year', 'month', 'ESACCI-sm', 'Percent_Snow', 'NDWI', 'PET', 'MODIS_LC', 'Ts', 'LST_Day',\n","                'LST_Night', 'Lai', 'Fpar', 'CSIF-SIFdaily', 'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN']\n","k=5\n","weights='uniform'\n","c=-1 # if impute_method = 'constant'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class PrepareMonthlyData:\n","    def __init__(self, included_features, monthly_data_input_fname, data_dir):\n","        self.included_features =included_features\n","        self.data_dir = data_dir\n","        self.monthly_data_input_fname = monthly_data_input_fname\n","        self.month_df = pd.read_csv(self.monthly_data_input_fname, usecols=self.included_features)\n","        self.month_df['date'] = pd.to_datetime(self.month_df['TIMESTAMP'],  format=\"%Y%m\")\n","\n","\n","    def to_datetime(self, row):\n","        return pd.to_datetime(f'{row.year}{row.month:02}', format='%Y%m')\n","\n","\n","    def knn_impute_site(self, df, knn_imp_cols, k, weights):\n","        # Fit and transform the data using KNNImputer, format as DF\n","        inds = df.index.copy()\n","        df_subcols = df[knn_imp_cols].copy()\n","        df_subcols = df_subcols.dropna(axis=1, how='all') # drop col if all NA, need to globally impute later\n","\n","        # Execute imputation\n","        imputer = KNNImputer(n_neighbors=k, weights=weights)\n","        imputed_group = imputer.fit_transform(df_subcols)\n","        imputed_group = pd.DataFrame(imputed_group, columns=df_subcols.columns, index=inds)\n","\n","        # Fill NA in initial site/group df\n","        df.fillna(imputed_group, inplace=True)\n","\n","        return df\n","\n","\n","    def impute(self, impute_method, resample_monthly, knn_imp_cols=None, k=None, weights=None, c=-1):\n","        # Resample to fill in missing month gaps, and interpolate values at site-level\n","        monthly_df = None\n","\n","        # Subset month_df to only sites with hourly records available\n","        available_sites = [x[-10:-4] for x in os.listdir(self.data_dir)]\n","        init_sites = len(self.month_df['SITE_ID'].unique())\n","        self.month_df = self.month_df.loc[self.month_df['SITE_ID'].isin(available_sites)]\n","        print(f\"# sites dropped bc not available in data_dir: {init_sites - len(self.month_df['SITE_ID'].unique())}\")\n","        \n","        # Loop through hourly site data to determine which months are present\n","        for i, s in tqdm(enumerate(self.month_df['SITE_ID'].unique())):\n","            # Get monthly data for site\n","            site_month = self.month_df[self.month_df['SITE_ID'] == s].copy()\n","            site_month.reset_index(drop = True, inplace=True)\n","            site_month['gap_flag_month'] = 0\n","\n","            if resample_monthly:\n","                # Get start and end range for given site <------------------------- CREATE DF NEXT TIME TO SAVE TIME (30 seconds per run)\n","                site_file = f'data_full_half_hourly_raw_v0_1_{s}.csv'\n","                site_hr_df = pd.read_csv(f\"{tmp_dir}/{site_file}\", usecols=['SITE_ID', 'datetime', 'year', 'month'])\n","                dates = [d for d in pd.date_range(start=site_hr_df['datetime'].min(), end=site_hr_df['datetime'].max(), freq='M')]\n","\n","                # Create dataframe\n","                site_hr_df = pd.DataFrame({'datetime': dates})\n","                site_hr_df['year'] = site_hr_df['datetime'].dt.year\n","                site_hr_df['month'] = site_hr_df['datetime'].dt.month\n","                site_hr_df['SITE_ID'] = s\n","\n","                # Resample montlhly data to get the months required in hourly data\n","                pft = site_month['MODIS_PFT'][0] # retain PFT to fill new rows\n","                site_month = pd.merge(site_hr_df, site_month, how='left', on =['SITE_ID', 'year', 'month'])\n","                site_month['MODIS_PFT'] = pft\n","                site_month['SITE_ID'] = s\n","                site_month['gap_flag_month'].fillna(1, inplace=True)\n","\n","            # Fill in known values for new/resampled month-level rows\n","            site_month['datetime'] = site_month.apply(self.to_datetime, axis=1)\n","            site_month.set_index('datetime', inplace=True)\n","            site_month.drop(columns='TIMESTAMP', inplace=True)\n","            site_month.drop(columns='date', inplace=True)\n","\n","            # If any new months added by resample, interpolate gap values at site-level\n","            if site_month.isna().sum().sum() != 0: \n","                if impute_method == 'interpolate':\n","                    site_month.interpolate(method='linear', limit_direction='both', inplace=True)\n","\n","                elif impute_method == 'knn':\n","                    site_month = self.knn_impute_site(site_month, knn_imp_cols, k, weights)\n","\n","                elif impute_method == 'constant':\n","                    monthly_df = self.month_df.fillna(c)\n","\n","            # Concat site_month to monthly_df\n","            if type(monthly_df) == type(None):\n","                monthly_df = site_month\n","            else:\n","                monthly_df = pd.concat([monthly_df, site_month])\n","\n","        # if any site had 100% missing for a feature, impute these using global data\n","        if monthly_df.isna().sum().sum() != 0:\n","            print(\"Imputing values where site has 100 percent of feature missing\")\n","            print(f\"# of NA features before global impute: {monthly_df.isna().sum().sum()}\")\n","            if impute_method == 'interpolate':\n","                monthly_df.interpolate(method='linear', limit_direction='both', inplace=True)\n","\n","            elif impute_method == 'knn':\n","                monthly_df = self.knn_impute_site(monthly_df, knn_imp_cols, k, weights)\n","\n","            elif impute_method == 'constant':\n","                monthly_df = self.monthly_df.fillna(c)\n","\n","            print(f\"# of NA features after global impute: {monthly_df.isna().sum().sum()}\")\n","\n","        return monthly_df\n","\n","\n","    def run(self, impute=False, impute_method=None, resample_monthly=False, knn_imp_cols=None, k=None, weights=None, c=-1):\n","        # Hanlde missing values\n","        if impute:\n","            print(f\"Impute method: {impute_method}\")\n","            print(f\"Resampling and gap filling missing months: {resample_monthly}\")\n","            monthly_df = self.impute(impute_method, resample_monthly, knn_imp_cols, k, weights, c)\n","        else:\n","            print(\"Not gap filling or filling NAs, leave be\")\n","            available_sites = [x[-10:-4] for x in os.listdir(self.data_dir)]\n","            self.month_df = self.month_df.loc[self.month_df['SITE_ID'].isin(available_sites)]\n","            monthly_df = self.month_df.copy()\n","            \n","        # Confirm No NAS\n","        if monthly_df.isna().sum().sum() == 0:\n","            print(\"Confirmed: No NA values remain\")\n","        elif type(impute_method) != type(None):\n","            print(\"ISSUE: SOME NA VALUES REMAIN - INVESTIGATE\")\n","            monthly_df.isna().sum()\n","\n","        return monthly_df\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Execute and Save Out"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Impute method: interpolate\n","Resampling and gap filling missing months: True\n","# sites dropped bc not available in data_dir: 9\n"]},{"name":"stderr","output_type":"stream","text":["234it [00:54,  4.29it/s]"]},{"name":"stdout","output_type":"stream","text":["Imputing values where site has 100 percent of feature missing\n","# of NA features before global impute: 1839\n","# of NA features after global impute: 0\n","Confirmed: No NA values remain\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Execute Monthly Preparation\n","PrepMonthly = PrepareMonthlyData(included_features, monthly_data_input_fname, tmp_dir)                                \n","monthly_df_out = PrepMonthly.run(impute, impute_method, resample_monthly, knn_imp_cols, k, weights, c)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Save out\n","monthly_df_out.to_csv(monthly_data_output_fname, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Issue with US-NR1 Months"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s = 'US-NR1'\n","site_file = f'data_full_half_hourly_raw_v0_1_{s}.csv'\n","site_hr_df = pd.read_csv(f\"{tmp_dir}/{site_file}\", usecols=['SITE_ID', 'datetime', 'year', 'month'])\n","dates = [d for d in pd.date_range(start=site_hr_df['datetime'].min(), end=site_hr_df['datetime'].max(), freq='M')]\n","\n","# Create dataframe\n","site_hr_df = pd.DataFrame({'datetime': dates})\n","site_hr_df['year'] = site_hr_df['datetime'].dt.year\n","site_hr_df['month'] = site_hr_df['datetime'].dt.month\n","\n","display(site_hr_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["usnr1 = monthly_df_out.loc[monthly_df_out['SITE_ID']=='US-NR1', ].copy()\n","usnr1.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Check Monthly Data for errors/overwriting of non-NA values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create pre and post copies to compare\n","df_imputed = monthly_df_out.copy().reset_index()\n","df_init = pd.read_csv(monthly_data_input_fname, usecols=included_features)\n","df_init['date'] = pd.to_datetime(df_init['TIMESTAMP'],  format=\"%Y%m\")\n","\n","# confirm no NAs remain in new df\n","na_rows_post = df_imputed.isna().any(axis=1).sum()\n","print(f\"Number of NA rows post imputation: {na_rows_post}\")\n","\n","# Drop NA rows from both (using indices) confirm they are same df now\n","drop_na = df_init.dropna(how='any')\n","drop_imp = df_imputed.loc[drop_na.index]\n","drop_na.reset_index(inplace=True, drop=True)\n","drop_imp.reset_index(inplace=True, drop=True)\n","shared_cols = list(set(drop_imp.columns).intersection(drop_na.columns))\n","print(f\"Are all rows with no NAs the same as before? {drop_na[shared_cols].equals(drop_imp[shared_cols])}\")\n","\n","# Check that 50 rows that initiall had NA are the same in non-NA cols\n","na_inds = df_init.loc[df_init.isna().any(axis=1), ].index\n","errors = 0\n","for ind in na_inds[:1000]:\n","    check_ind = pd.concat([df_init.iloc[ind], df_imputed.iloc[ind]], axis=1).dropna()\n","    check_ind.columns = ['initial', 'post_imp']\n","    if not check_ind['initial'].equals(check_ind['post_imp']):\n","        errors += 1\n","        print(ind)\n","print(f\"Number of non-NA values changed by error: {errors}\")\n","\n","# DF length is the same \n","print(f\"DF is same length as before: {len(df_init) == len(df_imputed)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Check Interpolation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loop through hourly site data to determine which months are present\n","s = 'AR-SLu'\n","resample_monthly = True\n","month_df = pd.read_csv(monthly_data_input_fname, usecols=included_features)\n","month_df['date'] = pd.to_datetime(month_df['TIMESTAMP'],  format=\"%Y%m\")\n","site_month = month_df[month_df['SITE_ID'] == s].copy()\n","site_month.reset_index(drop = True, inplace=True)\n","\n","def to_datetime(row):\n","    return pd.to_datetime(f'{row.year}{row.month:02}', format='%Y%m')\n","\n","if resample_monthly:\n","    # Get hourly data for site to find months to fill\n","    site_file = f'data_full_half_hourly_raw_v0_1_{s}.csv'\n","    try:\n","        site_hr_df = pd.read_csv(f\"{tmp_dir}/{site_file}\", usecols=['SITE_ID', 'year', 'month'])\n","    except:\n","        print(f\"{site_file} not available\")\n","\n","    # Get set of year-months represented in site-hourly dataset\n","    site_hr_df.drop_duplicates(inplace=True)\n","    site_hr_df['datetime'] = site_hr_df.apply(to_datetime, axis=1)\n","\n","    # Resample montlhly data to get the months required in hourly data\n","    pft = site_month['MODIS_PFT'][0] # retain PFT to fill new rows\n","    site_month = pd.merge(site_hr_df, site_month, how='left', on =['SITE_ID', 'year', 'month'])\n","    site_month['MODIS_PFT'] = pft\n","    site_month['SITE_ID'] = s\n","\n","# Fill in known values for new/resampled month-level rows\n","site_month['datetime'] = site_month.apply(to_datetime, axis=1)\n","site_month.set_index('datetime', inplace=True)\n","site_month.drop(columns='TIMESTAMP', inplace=True)\n","site_month.drop(columns='date', inplace=True)\n","\n","na_inds = site_month[site_month.isna().any(axis=1)].index\n","print(na_inds[:2])\n","\n","# If any new months added by resample, interpolate gap values at site-level\n","if site_month.isna().sum().sum() != 0: \n","    site_month.interpolate(method='linear', limit_direction='both', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["site_month.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMksgG6dJySdU68N+q2MrXO","collapsed_sections":["4-mBiDcbrZ1f","nuowBABbrhti","eHPfBKGasKAQ"],"provenance":[]},"kernelspec":{"display_name":"src-rYFmMrY7-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"}}},"nbformat":4,"nbformat_minor":0}
