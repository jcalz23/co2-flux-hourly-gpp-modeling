{"cells":[{"cell_type":"markdown","metadata":{"id":"4-mBiDcbrZ1f"},"source":["# Notebook Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":883,"status":"ok","timestamp":1676804752494,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"0vOPrR8CrO1r","outputId":"97955b62-22e1-4653-cdb5-0b475838d897"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  IN_COLLAB = True\n","else:\n","  IN_COLLAB = False\n","\n","#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n","MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n","\n","if IN_COLLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"nuowBABbrhti"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1376,"status":"ok","timestamp":1676804760778,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"noZwsViCrnAS"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install required modules quietly\n","required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n","\n","for p in required_packages: \n","  try:\n","      __import__(p)\n","  except ImportError:\n","      %pip install {p} --quiet\n","\n","import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","os.chdir(MY_HOME_ABS_PATH) # <------------------ ADDED\n","import math\n","import json\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from pyspark.sql.functions import col\n","import pyspark.pandas as pd\n","from calendar import monthrange\n","from datetime import datetime\n","from io import BytesIO\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load locale custome modules\n","import sys\n","if IN_COLLAB:\n","  os.chdir(MY_HOME_ABS_PATH)\n","  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n","else:\n","  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n","\n","from CloudIO.AzStorageClient import AzStorageClient\n","from data_pipeline_lib import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)\n","\n","\n","# Define paths\n","root_dir =  MY_HOME_ABS_PATH\n","tmp_dir =  root_dir + os.sep + '.tmp'\n","raw_data_dir = root_dir + os.sep + 'data'\n","data_dir = root_dir + os.sep + 'data/datasets'\n","cred_dir = root_dir + os.sep + '.cred'\n","az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n","if IN_COLLAB:\n","  raw_data_dir = \"/content/drive/MyDrive/CO2_flux_gpp_modeling/DS_capstone_23Spring_CO2/Data\""]},{"cell_type":"markdown","metadata":{"id":"2IwvFVLFrtkM"},"source":["# Constant Definitions"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1676804775178,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"KXgevZgWrxiV"},"outputs":[],"source":["# Select monthly features to use\n","included_features= ['SITE_ID', 'year', 'month', 'TIMESTAMP',\n","                   'ESACCI-sm',    # ESACCI Soil Moisture (%)\n","                   'Percent_Snow', # Percentage of snow cover (%)\n","                   'NDWI',      # Normalized Different Water Index (NDWI)\n","                   'PET',       # Potential ET (m)\n","                   'MODIS_PFT', # Plant Function Type\n","                   'MODIS_LC',  # MODIS Land Cover\n","                   'Ts',        # Skin temperature (K) ??\n","                   'LST_Day',   # Daytime land surface temperature (K)\n","                   'LST_Night', # Nightime land surface temperature (K)\n","                   'Lai',       # Leaf Area Index (LAI)\n","                   'Fpar',      # Fraction of photosynthetically active radiation (fPAR)\n","                   'CSIF-SIFdaily', # All-sky daily average SIF\n","                   'BESS-PAR',      # Photosynthetic Active Radiation (PAR) (W/m^2)\n","                   'BESS-PARdiff',  # Diffuse PAR (W/m^2)\n","                   'BESS-RSDN'      # Shortwave downwelling radiation (W/m^2)\n","                   ]\n","\n","# Define in and out files for monthly data\n","monthly_data_input_fname = data_dir + os.sep + 'data_monthly_v1_0.csv'\n","monthly_data_output_fname = raw_data_dir + os.sep + \"monthly-interpolated-v3.csv\"\n","\n","# Define methods for filling NA (interpolate or -1) and gap-fill (fill or leave be)\n","resample_monthly = False\n","impute_method = '-1' # other options are 'interpolate' or None"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["class PrepareMonthlyData:\n","    def __init__(self, included_features, monthly_data_input_fname, impute_method='-1', resample_monthly=False):\n","        self.impute_method = impute_method\n","        self.resample_monthly = resample_monthly\n","        self.included_features =included_features\n","        self.monthly_data_input_fname = monthly_data_input_fname\n","        self.month_df = pd.read_csv(self.monthly_data_input_fname, usecols=self.included_features)\n","        self.month_df['date'] = pd.to_datetime(self.month_df['TIMESTAMP'],  format=\"%Y%m\")\n","\n","\n","    def to_datetime(self, row):\n","        return pd.to_datetime(f'{row.year}{row.month:02}', format='%Y%m')\n","\n","\n","    def interpolate(self):\n","        # Resample to fill in missing month gaps, and interpolate values at site-level\n","        monthly_df = None  \n","\n","        # Loop through hourly site data to determine which months are present\n","        for i, s in tqdm(enumerate(self.month_df['SITE_ID'].unique())):\n","            # Get monthly data for site\n","            site_month = self.month_df[self.month_df['SITE_ID'] == s].copy()\n","            site_month.reset_index(drop = True, inplace=True)\n","\n","            if self.resample_monthly:\n","                # Get hourly data for site to find months to fill\n","                site_file = f'data_full_half_hourly_raw_v0_1_{s}.csv'\n","                try:\n","                    site_hr_df = pd.read_csv(f\"{tmp_dir}/{site_file}\", usecols=['SITE_ID', 'year', 'month'])\n","                except:\n","                    print(f\"{site_file} not available\")\n","                    continue\n","\n","                # Get set of year-months represented in site-hourly dataset\n","                site_hr_df.drop_duplicates(inplace=True)\n","                site_hr_df['datetime'] = site_hr_df.apply(self.to_datetime, axis=1)\n","\n","                # Resample montlhly data to get the months required in hourly data\n","                pft = site_month['MODIS_PFT'][0] # retain PFT to fill new rows\n","                site_month = pd.merge(site_hr_df, site_month, how='left', on =['SITE_ID', 'year', 'month'])\n","                site_month['MODIS_PFT'] = pft\n","                site_month['SITE_ID'] = s\n","\n","            # Fill in known values for new/resampled month-level rows\n","            site_month['datetime'] = site_month.apply(self.to_datetime, axis=1)\n","            site_month.set_index('datetime', inplace=True)\n","            site_month.drop(columns='TIMESTAMP', inplace=True)\n","            site_month.drop(columns='date', inplace=True)\n","\n","            # If any new months added by resample, interpolate gap values at site-level\n","            if site_month.isna().sum().sum() != 0: \n","                site_month.interpolate(method='linear', limit_direction='both', inplace=True)\n","\n","            # Concat to monthly_df across sites\n","            if type(monthly_df) == type(None):\n","                monthly_df = site_month\n","            else:\n","                monthly_df = pd.concat([monthly_df, site_month])\n","\n","        # if any site had 100% missing for a feature, impute these using global data\n","        if monthly_df.isna().sum().sum() != 0: \n","            print(\"Interpolating Values for 100% Missing Records\")\n","            monthly_df.interpolate(method='linear', limit_direction='both', inplace=True)\n","\n","        return monthly_df\n","\n","\n","    def run(self):\n","        # fill in missing months or leave it\n","        if self.impute_method == 'interpolate':\n","            print(\"Interpolating Values for Missing Values\")\n","            if self.resample_monthly: print(\"Gap-Filling Missing Months\")\n","            monthly_df = self.interpolate()\n","        elif self.impute_method == '-1':\n","            print(\"No Gap Filling of Monthly Data, Filling all NA with -1\")\n","            monthly_df = self.month_df.fillna(-1)\n","        else:\n","            monthly_df = self.month_df.copy()\n","            print(\"Not filling NAs, leave be\")\n","\n","        # Confirm No NAS\n","        if monthly_df.isna().sum().sum() == 0:\n","            print(\"Confirmed: No NA values remain\")\n","        elif type(self.impute_method) != type(None):\n","            print(\"ISSUE: SOME NA VALUES REMAIN - INVESTIGATE\")\n","            monthly_df.isna().sum()\n","\n","        return monthly_df\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Execute and Save Out"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No Gap Filling of Monthly Data, Filling all NA with -1\n","Confirmed: No NA values remain\n"]}],"source":["# Execute Monthly Preparation\n","PrepMonthly = PrepareMonthlyData(included_features, monthly_data_input_fname, impute_method, resample_monthly)                                \n","monthly_df_out = PrepMonthly.run()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Save out\n","monthly_df_out.to_csv(monthly_data_output_fname, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Check Monthly Data for errors/overwriting of non-NA values"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of NA rows post imputation: 0\n","Are all rows with no NAs the same as before? True\n","Number of non-NA values changed by error: 0\n","DF is same length as before: True\n"]}],"source":["# Create pre and post copies to compare\n","df_imputed = monthly_df_out.copy()\n","df_init = pd.read_csv(monthly_data_input_fname, usecols=included_features)\n","df_init['date'] = pd.to_datetime(df_init['TIMESTAMP'],  format=\"%Y%m\")\n","\n","# confirm no NAs remain in new df\n","na_rows_post = df_imputed.isna().any(axis=1).sum()\n","print(f\"Number of NA rows post imputation: {na_rows_post}\")\n","\n","# Drop NA rows from both (using indices) confirm they are same df now\n","drop_na = df_init.dropna(how='any')\n","drop_imp = df_imputed.loc[drop_na.index, ]\n","drop_na.reset_index(inplace=True, drop=True)\n","drop_imp.reset_index(inplace=True, drop=True)\n","print(f\"Are all rows with no NAs the same as before? {drop_na.equals(drop_imp)}\")\n","\n","# Check that 50 rows that initiall had NA are the same in non-NA cols\n","na_inds = df_init.loc[df_init.isna().any(axis=1), ].index\n","errors = 0\n","for ind in na_inds[:50]:\n","    check_ind = pd.concat([df_init.iloc[ind], df_imputed.iloc[ind]], axis=1).dropna()\n","    check_ind.columns = ['initial', 'post_imp']\n","    if not check_ind['initial'].equals(check_ind['post_imp']):\n","        errors += 1\n","        print(ind)\n","print(f\"Number of non-NA values changed by error: {errors}\")\n","\n","# DF length is the same \n","print(f\"DF is same length as before: {len(df_init) == len(df_imputed)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Check Interpolation"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatetimeIndex(['2009-12-01'], dtype='datetime64[ns]', name='datetime', freq=None)\n"]}],"source":["# Loop through hourly site data to determine which months are present\n","s = 'AR-SLu'\n","resample_monthly = True\n","month_df = pd.read_csv(monthly_data_input_fname, usecols=included_features)\n","month_df['date'] = pd.to_datetime(month_df['TIMESTAMP'],  format=\"%Y%m\")\n","site_month = month_df[month_df['SITE_ID'] == s].copy()\n","site_month.reset_index(drop = True, inplace=True)\n","\n","def to_datetime(row):\n","    return pd.to_datetime(f'{row.year}{row.month:02}', format='%Y%m')\n","\n","if resample_monthly:\n","    # Get hourly data for site to find months to fill\n","    site_file = f'data_full_half_hourly_raw_v0_1_{s}.csv'\n","    try:\n","        site_hr_df = pd.read_csv(f\"{tmp_dir}/{site_file}\", usecols=['SITE_ID', 'year', 'month'])\n","    except:\n","        print(f\"{site_file} not available\")\n","\n","    # Get set of year-months represented in site-hourly dataset\n","    site_hr_df.drop_duplicates(inplace=True)\n","    site_hr_df['datetime'] = site_hr_df.apply(to_datetime, axis=1)\n","\n","    # Resample montlhly data to get the months required in hourly data\n","    pft = site_month['MODIS_PFT'][0] # retain PFT to fill new rows\n","    site_month = pd.merge(site_hr_df, site_month, how='left', on =['SITE_ID', 'year', 'month'])\n","    site_month['MODIS_PFT'] = pft\n","    site_month['SITE_ID'] = s\n","\n","# Fill in known values for new/resampled month-level rows\n","site_month['datetime'] = site_month.apply(to_datetime, axis=1)\n","site_month.set_index('datetime', inplace=True)\n","site_month.drop(columns='TIMESTAMP', inplace=True)\n","site_month.drop(columns='date', inplace=True)\n","\n","na_inds = site_month[site_month.isna().any(axis=1)].index\n","print(na_inds[:2])\n","\n","# If any new months added by resample, interpolate gap values at site-level\n","if site_month.isna().sum().sum() != 0: \n","    site_month.interpolate(method='linear', limit_direction='both', inplace=True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>SITE_ID</th>\n","      <th>BESS-PAR</th>\n","      <th>BESS-PARdiff</th>\n","      <th>BESS-RSDN</th>\n","      <th>CSIF-SIFdaily</th>\n","      <th>PET</th>\n","      <th>Ts</th>\n","      <th>ESACCI-sm</th>\n","      <th>MODIS_LC</th>\n","      <th>NDWI</th>\n","      <th>Percent_Snow</th>\n","      <th>Fpar</th>\n","      <th>Lai</th>\n","      <th>LST_Day</th>\n","      <th>LST_Night</th>\n","      <th>MODIS_PFT</th>\n","    </tr>\n","    <tr>\n","      <th>datetime</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2009-12-01</th>\n","      <td>2009</td>\n","      <td>12</td>\n","      <td>AR-SLu</td>\n","      <td>154.00000</td>\n","      <td>40.00000</td>\n","      <td>336.00000</td>\n","      <td>0.20432</td>\n","      <td>-0.01339</td>\n","      <td>302.46967</td>\n","      <td>0.15152</td>\n","      <td>7.00000</td>\n","      <td>0.03542</td>\n","      <td>0.00000</td>\n","      <td>0.49000</td>\n","      <td>1.20000</td>\n","      <td>313.84000</td>\n","      <td>293.58000</td>\n","      <td>SH</td>\n","    </tr>\n","    <tr>\n","      <th>2010-01-01</th>\n","      <td>2010</td>\n","      <td>1</td>\n","      <td>AR-SLu</td>\n","      <td>154.00000</td>\n","      <td>40.00000</td>\n","      <td>336.00000</td>\n","      <td>0.20432</td>\n","      <td>-0.01339</td>\n","      <td>302.46967</td>\n","      <td>0.15152</td>\n","      <td>7.00000</td>\n","      <td>0.03542</td>\n","      <td>0.00000</td>\n","      <td>0.49000</td>\n","      <td>1.20000</td>\n","      <td>313.84000</td>\n","      <td>293.58000</td>\n","      <td>SH</td>\n","    </tr>\n","    <tr>\n","      <th>2010-02-01</th>\n","      <td>2010</td>\n","      <td>2</td>\n","      <td>AR-SLu</td>\n","      <td>120.00000</td>\n","      <td>46.00000</td>\n","      <td>258.00000</td>\n","      <td>0.14553</td>\n","      <td>-0.00894</td>\n","      <td>298.78864</td>\n","      <td>0.16656</td>\n","      <td>7.00000</td>\n","      <td>0.00040</td>\n","      <td>0.00000</td>\n","      <td>0.43000</td>\n","      <td>0.90000</td>\n","      <td>309.86000</td>\n","      <td>292.96000</td>\n","      <td>SH</td>\n","    </tr>\n","    <tr>\n","      <th>2010-03-01</th>\n","      <td>2010</td>\n","      <td>3</td>\n","      <td>AR-SLu</td>\n","      <td>107.00000</td>\n","      <td>31.00000</td>\n","      <td>231.00000</td>\n","      <td>0.10980</td>\n","      <td>-0.00813</td>\n","      <td>297.54816</td>\n","      <td>0.16408</td>\n","      <td>7.00000</td>\n","      <td>-0.02286</td>\n","      <td>0.00000</td>\n","      <td>0.41000</td>\n","      <td>0.80000</td>\n","      <td>309.18000</td>\n","      <td>290.52000</td>\n","      <td>SH</td>\n","    </tr>\n","    <tr>\n","      <th>2010-04-01</th>\n","      <td>2010</td>\n","      <td>4</td>\n","      <td>AR-SLu</td>\n","      <td>81.00000</td>\n","      <td>27.00000</td>\n","      <td>175.00000</td>\n","      <td>0.07673</td>\n","      <td>-0.00676</td>\n","      <td>291.69604</td>\n","      <td>0.12402</td>\n","      <td>7.00000</td>\n","      <td>-0.04202</td>\n","      <td>0.00000</td>\n","      <td>0.36000</td>\n","      <td>0.50000</td>\n","      <td>303.24000</td>\n","      <td>286.34000</td>\n","      <td>SH</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            year  month SITE_ID  BESS-PAR  BESS-PARdiff  BESS-RSDN  \\\n","datetime                                                             \n","2009-12-01  2009     12  AR-SLu 154.00000      40.00000  336.00000   \n","2010-01-01  2010      1  AR-SLu 154.00000      40.00000  336.00000   \n","2010-02-01  2010      2  AR-SLu 120.00000      46.00000  258.00000   \n","2010-03-01  2010      3  AR-SLu 107.00000      31.00000  231.00000   \n","2010-04-01  2010      4  AR-SLu  81.00000      27.00000  175.00000   \n","\n","            CSIF-SIFdaily      PET        Ts  ESACCI-sm  MODIS_LC     NDWI  \\\n","datetime                                                                     \n","2009-12-01        0.20432 -0.01339 302.46967    0.15152   7.00000  0.03542   \n","2010-01-01        0.20432 -0.01339 302.46967    0.15152   7.00000  0.03542   \n","2010-02-01        0.14553 -0.00894 298.78864    0.16656   7.00000  0.00040   \n","2010-03-01        0.10980 -0.00813 297.54816    0.16408   7.00000 -0.02286   \n","2010-04-01        0.07673 -0.00676 291.69604    0.12402   7.00000 -0.04202   \n","\n","            Percent_Snow    Fpar     Lai   LST_Day  LST_Night MODIS_PFT  \n","datetime                                                                 \n","2009-12-01       0.00000 0.49000 1.20000 313.84000  293.58000        SH  \n","2010-01-01       0.00000 0.49000 1.20000 313.84000  293.58000        SH  \n","2010-02-01       0.00000 0.43000 0.90000 309.86000  292.96000        SH  \n","2010-03-01       0.00000 0.41000 0.80000 309.18000  290.52000        SH  \n","2010-04-01       0.00000 0.36000 0.50000 303.24000  286.34000        SH  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["site_month.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMksgG6dJySdU68N+q2MrXO","collapsed_sections":["4-mBiDcbrZ1f","nuowBABbrhti","eHPfBKGasKAQ"],"provenance":[]},"kernelspec":{"display_name":"src-rYFmMrY7-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"}}},"nbformat":4,"nbformat_minor":0}
