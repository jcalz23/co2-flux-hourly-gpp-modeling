{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation and Gap-Filling Logic (Dev)\n",
    "Goal: To build a set of functions to impute and gap-fill site-level data in the pipeline process\n",
    "\n",
    "Imputation Logic: Use KNNImputer to find k most similar neighbors to missing point and impute with average of neighbor values\n",
    "Gap-Filling Logic: TBD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install required modules quietly\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "import math\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.pandas as pd\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load locale custome modules\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../tools\"))\n",
    "\n",
    "from CloudIO.AzStorageClient import AzStorageClient\n",
    "from data_pipeline_lib import *\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n",
    "\n",
    "for p in required_packages: \n",
    "  try:\n",
    "      __import__(p)\n",
    "  except ImportError:\n",
    "      %pip install {p} --quiet\n",
    "\n",
    "MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n",
    "root_dir =  MY_HOME_ABS_PATH\n",
    "tmp_dir =  root_dir + os.sep + '.tmp'\n",
    "raw_data_dir = tmp_dir\n",
    "data_dir = root_dir + os.sep + 'data'\n",
    "cred_dir = root_dir + os.sep + '.cred'\n",
    "az_cred_file = cred_dir + os.sep + 'azblobcred.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare One Site Dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variables of the data pipelines\n",
    "included_features = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA',\n",
    "                     'datetime', 'year', 'month', 'day', 'hour', 'date',\n",
    "                     'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', \n",
    "                     'IGBP', 'koppen']\n",
    "target_variable_qc = 'NEE_VUT_REF_QC'\n",
    "target_variable = 'GPP_NT_VUT_REF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>elevation</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>koppen_sub</th>\n",
       "      <th>koppen_main</th>\n",
       "      <th>koppen_name</th>\n",
       "      <th>c3c4</th>\n",
       "      <th>c4_percent</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN-HaM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.37000</td>\n",
       "      <td>101.18000</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>ET</td>\n",
       "      <td>C3</td>\n",
       "      <td>1.07000</td>\n",
       "      <td>data_full_half_hourly_raw_v0_1_CN-HaM.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_id  elevation      lat      long  koppen_sub  koppen_main koppen_name  \\\n",
       "0  CN-HaM        NaN 37.37000 101.18000          29            5          ET   \n",
       "\n",
       "  c3c4  c4_percent                                   filename  \n",
       "0   C3     1.07000  data_full_half_hourly_raw_v0_1_CN-HaM.csv  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick site, create dummy df\n",
    "site = 'CN-HaM' # <--- reduced to one site by John\n",
    "\n",
    "# Load site metadata\n",
    "included_site_features = ['site_id', 'filename', 'elevation', 'lat', 'long',\n",
    "                          'koppen_sub', 'koppen_main', 'koppen_name',\n",
    "                          'c3c4', 'c4_percent']\n",
    "                          \n",
    "site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'\n",
    "site_metadata_df = pd.read_csv(site_metadata_filename, usecols = included_site_features)\n",
    "\n",
    "# only focus on target sites\n",
    "site_metadata_df = site_metadata_df.loc[site_metadata_df['site_id'].isin([site])]\n",
    "print(f\"size:{site_metadata_df.shape}\")\n",
    "site_metadata_df.reset_index(inplace=True, drop=True)\n",
    "site_metadata_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Site w/ cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN-HaM: (12726, 27)\n",
      "Data size after cleanup: (12726, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPP_NT_VUT_REF</th>\n",
       "      <th>TA_ERA</th>\n",
       "      <th>SW_IN_ERA</th>\n",
       "      <th>LW_IN_ERA</th>\n",
       "      <th>VPD_ERA</th>\n",
       "      <th>P_ERA</th>\n",
       "      <th>PA_ERA</th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>date</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>NIRv</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>IGBP</th>\n",
       "      <th>koppen</th>\n",
       "      <th>minute</th>\n",
       "      <th>site_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.00263</td>\n",
       "      <td>-13.85300</td>\n",
       "      <td>53.56000</td>\n",
       "      <td>224.62300</td>\n",
       "      <td>0.51500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>68.68300</td>\n",
       "      <td>2002-01-15 09:00:00</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>2002-01-15</td>\n",
       "      <td>-0.03115</td>\n",
       "      <td>-0.00702</td>\n",
       "      <td>-0.00640</td>\n",
       "      <td>0.92520</td>\n",
       "      <td>0.91230</td>\n",
       "      <td>0.85710</td>\n",
       "      <td>0.91130</td>\n",
       "      <td>0.56010</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.07080</td>\n",
       "      <td>GRA</td>\n",
       "      <td>Polar</td>\n",
       "      <td>0</td>\n",
       "      <td>CN-HaM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.05477</td>\n",
       "      <td>-13.54500</td>\n",
       "      <td>121.26800</td>\n",
       "      <td>224.62300</td>\n",
       "      <td>0.58300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>68.73400</td>\n",
       "      <td>2002-01-15 10:00:00</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2002-01-15</td>\n",
       "      <td>-0.03115</td>\n",
       "      <td>-0.00702</td>\n",
       "      <td>-0.00640</td>\n",
       "      <td>0.92520</td>\n",
       "      <td>0.91230</td>\n",
       "      <td>0.85710</td>\n",
       "      <td>0.91130</td>\n",
       "      <td>0.56010</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.07080</td>\n",
       "      <td>GRA</td>\n",
       "      <td>Polar</td>\n",
       "      <td>0</td>\n",
       "      <td>CN-HaM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54304</td>\n",
       "      <td>-13.23700</td>\n",
       "      <td>301.45000</td>\n",
       "      <td>223.17500</td>\n",
       "      <td>0.65100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>68.78400</td>\n",
       "      <td>2002-01-15 11:00:00</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>2002-01-15</td>\n",
       "      <td>-0.03115</td>\n",
       "      <td>-0.00702</td>\n",
       "      <td>-0.00640</td>\n",
       "      <td>0.92520</td>\n",
       "      <td>0.91230</td>\n",
       "      <td>0.85710</td>\n",
       "      <td>0.91130</td>\n",
       "      <td>0.56010</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.07080</td>\n",
       "      <td>GRA</td>\n",
       "      <td>Polar</td>\n",
       "      <td>0</td>\n",
       "      <td>CN-HaM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GPP_NT_VUT_REF    TA_ERA  SW_IN_ERA  LW_IN_ERA  VPD_ERA   P_ERA   PA_ERA  \\\n",
       "0        -3.00263 -13.85300   53.56000  224.62300  0.51500 0.00000 68.68300   \n",
       "1        -2.05477 -13.54500  121.26800  224.62300  0.58300 0.00000 68.73400   \n",
       "2         0.54304 -13.23700  301.45000  223.17500  0.65100 0.00000 68.78400   \n",
       "\n",
       "             datetime  year  month  day  hour       date      EVI     NDVI  \\\n",
       "0 2002-01-15 09:00:00  2002      1   15     9 2002-01-15 -0.03115 -0.00702   \n",
       "1 2002-01-15 10:00:00  2002      1   15    10 2002-01-15 -0.03115 -0.00702   \n",
       "2 2002-01-15 11:00:00  2002      1   15    11 2002-01-15 -0.03115 -0.00702   \n",
       "\n",
       "      NIRv      b1      b2      b3      b4      b5      b6      b7 IGBP  \\\n",
       "0 -0.00640 0.92520 0.91230 0.85710 0.91130 0.56010 0.16090 0.07080  GRA   \n",
       "1 -0.00640 0.92520 0.91230 0.85710 0.91130 0.56010 0.16090 0.07080  GRA   \n",
       "2 -0.00640 0.92520 0.91230 0.85710 0.91130 0.56010 0.16090 0.07080  GRA   \n",
       "\n",
       "  koppen  minute site_id  \n",
       "0  Polar       0  CN-HaM  \n",
       "1  Polar       0  CN-HaM  \n",
       "2  Polar       0  CN-HaM  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load site data\n",
    "def data_cleanup(data_dir, site_id_file_df, target, target_qc, features):\n",
    "  data_df = None\n",
    "  # qc_flag_dtype = CategoricalDtype([0, 1, 2, 3], ordered=True)\n",
    "  qc_flags_features = [s for s in features if \"_QC\" in s]\n",
    "\n",
    "  # Iterate through each site:\n",
    "  for i, r in site_id_file_df.iterrows():        \n",
    "    if not r.filename or type(r.filename) != type(\"\"):\n",
    "      print(f'\\nERROR: {r.site_id} is mssing hourly data.')\n",
    "      continue\n",
    "\n",
    "    # Get only `features` from file\n",
    "    local_filename = data_dir + os.sep + r.filename\n",
    "    site_df = pd.read_csv(local_filename, usecols = [target, target_qc] + features)\n",
    "    site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n",
    "    site_df['date'] = pd.to_datetime(site_df['date'])\n",
    "    site_df['minute'] = site_df['datetime'].dt.minute\n",
    "    if len(qc_flags_features) != 0:\n",
    "      site_df[qc_flags_features] = site_df[qc_flags_features].astype('int')\n",
    "    site_df['site_id'] = r.site_id\n",
    "\n",
    "    # Remove zero or negative SW\n",
    "    site_df.drop(site_df[site_df['SW_IN_ERA'] <= 0].index, inplace = True)\n",
    "\n",
    "    # Drop rows with NAs for Target Variable\n",
    "    site_df.dropna(subset=[target], axis=0, inplace=True)\n",
    "\n",
    "    # Drop rows with bad NEE_VUT_REF_QC (aka bad GPP records)\n",
    "    site_df.drop(site_df[site_df[target_qc] == 3].index, inplace = True)\n",
    "    site_df.drop([target_qc], axis=1, inplace=True)\n",
    "\n",
    "    # Drop rows with any NA\n",
    "    #site_df.dropna(axis=0, inplace=True) # <---------------- REMOVED BY JOHN\n",
    "\n",
    "    # Move from HH to H level <---------------- ADDED BY JOHN\n",
    "    site_df = site_df.loc[site_df['minute']==0, ].copy()\n",
    "\n",
    "    print(f\"{r.site_id}: {site_df.shape}\")\n",
    "    if type(data_df) == type(None):\n",
    "      data_df = site_df\n",
    "    else:\n",
    "      data_df = pd.concat([data_df, site_df])\n",
    "          \n",
    "  return data_df\n",
    "\n",
    "# Initial data clean and feature selections from raw data\n",
    "data_df = data_cleanup(raw_data_dir, site_metadata_df,\n",
    "                  target_variable, target_variable_qc,\n",
    "                  included_features)\n",
    "print(f\"Data size after cleanup: {data_df.shape}\")\n",
    "\n",
    "# # Merge with site metadata\n",
    "# data_df = merge_site_metadata(data_df, site_metadata_df.drop(['filename', 'koppen_main', 'koppen_name'], axis=1))\n",
    "# print(f\"Data size after after merged with site metadata: {data_df.shape}\")\n",
    "\n",
    "# Drop rows with NA\n",
    "# check_and_drop_na(data_df) <---------------- REMOVED BY JOHN\n",
    "#print(f\"Data size after after final drop: {data_df.shape}\")\n",
    "\n",
    "#reorder columns\n",
    "features = data_df.columns.to_list()\n",
    "features.remove(target_variable)\n",
    "data_df = data_df[([target_variable] + features)]\n",
    "\n",
    "data_df.reset_index(inplace=True, drop=True)  #<---------------- ADDED BY JOHN\n",
    "\n",
    "display(data_df.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categoricals for KNN only\n",
    "Decision to make: Do we treat month, hour as numerical, or use Season and TOD groupings? The question is how will KNN use the time information to identify similar records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add season and TOD groupings\n",
    "def define_season_tod(df):\n",
    "    # Season indicator\n",
    "    df.loc[df['month'].isin([12, 1, 2]), 'season'] = 'WINTER'\n",
    "    df.loc[df['month'].isin([3, 4, 5]), 'season'] = 'SPRING'\n",
    "    df.loc[df['month'].isin([6, 7, 8]), 'season'] = 'SUMMER'\n",
    "    df.loc[df['month'].isin([9, 10, 11]), 'season'] = 'FALL'\n",
    "\n",
    "    # 6-Hour TOD indicator\n",
    "    df.loc[df['hour'].isin(list(range(0, 6))), 'time_block'] = '1'\n",
    "    df.loc[df['hour'].isin(list(range(6, 12))), 'time_block'] = '2'\n",
    "    df.loc[df['hour'].isin(list(range(12, 18))), 'time_block'] = '3'\n",
    "    df.loc[df['hour'].isin(list(range(18, 24))), 'time_block'] = '4'\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Encode categorical features for KNN Imputer\n",
    "def knn_impute(df, knn_imp_cat, knn_imp_real, k=5):\n",
    "    # Add Season, TOD features <------ only use if we decide to use these as time features\n",
    "    df = define_season_tod(df)\n",
    "\n",
    "    # One-hot encoding\n",
    "    knn_df = df[knn_imp_cat + knn_imp_real].copy()\n",
    "    knn_df = pd.get_dummies(knn_df,\n",
    "                            columns=knn_imp_cat, \n",
    "                            dummy_na=False)\n",
    "\n",
    "    # Initialize KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='uniform') #<-- may want to try weighted by distance later\n",
    "\n",
    "    # Fit and transform the data using KNNImputer\n",
    "    imputed_data = imputer.fit_transform(knn_df)\n",
    "\n",
    "    # Convert the imputed data back to a DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=knn_df.columns)\n",
    "\n",
    "    # update initial df with imputed values\n",
    "    data_df_imp = df.copy()\n",
    "    data_df_imp.update(imputed_df, overwrite=True)\n",
    "\n",
    "    return data_df_imp\n",
    "\n",
    "\n",
    "# Define the features to use in KNN imputer\n",
    "knn_imp_cat = ['season', 'time_block']\n",
    "knn_imp_real = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA', \n",
    "                'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7']\n",
    "data_df_imp = knn_impute(data_df, knn_imp_cat, knn_imp_real)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECK IMPUTATION\n",
    "Double-check that the imputed data update didn't affect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA rows post imputation: 0\n",
      "Are rows with no NAs the same as before? True\n",
      "Number of non-NA values changed: 0\n"
     ]
    }
   ],
   "source": [
    "## Compare data_df init to data_df_copy with filled NA values\n",
    "# confirm no NAs remain in new df\n",
    "na_rows_post = data_df_imp.isna().any(axis=1).sum()\n",
    "print(f\"Number of NA rows post imputation: {na_rows_post}\")\n",
    "\n",
    "# Drop NA rows from both (using indices) confirm they are same df now\n",
    "drop_na = data_df.dropna(how='any')\n",
    "drop_imp = data_df_imp.iloc[drop_na.index, ]\n",
    "print(f\"Are rows with no NAs the same as before? {drop_na.equals(drop_imp)}\")\n",
    "\n",
    "# Check that some rows with NA are the same in non-NA cols\n",
    "na_inds = data_df.loc[data_df.isna().any(axis=1), ].index\n",
    "errors = 0\n",
    "for ind in na_inds:\n",
    "    check_ind = pd.concat([data_df.iloc[ind], data_df_imp.iloc[ind]], axis=1).dropna()\n",
    "    check_ind.columns = ['initial', 'post_imp']\n",
    "    if not check_ind['initial'].equals(check_ind['post_imp']):\n",
    "        errors += 1\n",
    "        print(ind)\n",
    "\n",
    "print(f\"Number of non-NA values changed by error: {errors}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Missing Vals (more later, plus move up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate how many rows have any NA <---- Added\n",
    "# cols_with_all_na = data_df.columns[data_df.isna().all()]\n",
    "# print(f\"Dropping {cols_with_all_na} due to 100% missing\")\n",
    "# data_df.drop(cols_with_all_na, axis=1, inplace=True)\n",
    "# num_rows_with_na = data_df.isna().any(axis=1).sum()\n",
    "# print(f\"Rows with any NA values: Count = {num_rows_with_na}, % = {100*round(num_rows_with_na/len(data_df), 2):.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-rYFmMrY7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
