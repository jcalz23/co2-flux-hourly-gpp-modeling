{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n",
    "MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLLAB = True\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive/')\n",
    "else:\n",
    "  IN_COLLAB = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required modules quietly\n",
    "required_packages = ['azure-storage-blob']\n",
    "\n",
    "for p in required_packages: \n",
    "  try:\n",
    "      __import__(p)\n",
    "  except ImportError:\n",
    "      %pip install {p} --quiet\n",
    "\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load locale custome modules\n",
    "os.chdir(MY_HOME_ABS_PATH)\n",
    "if IN_COLLAB:\n",
    "  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n",
    "else:\n",
    "  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n",
    "\n",
    "from CloudIO.AzStorageClient import AzStorageClient\n",
    "from data_pipeline_lib import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "root_dir =  MY_HOME_ABS_PATH\n",
    "tmp_dir =  root_dir + os.sep + '.tmp'\n",
    "raw_data_dir = tmp_dir\n",
    "data_dir = root_dir + os.sep + 'data'\n",
    "cred_dir = root_dir + os.sep + '.cred'\n",
    "az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n",
    "preproc_objects_dir = root_dir + os.sep + 'code' + os.sep + 'src' + os.sep + 'preprocessing' + os.sep + 'preproc_objects'\n",
    "\n",
    "# input files\n",
    "site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'\n",
    "\n",
    "# Azure container, file names\n",
    "container = \"all-sites-data\"\n",
    "ver = \"mvp\"\n",
    "tag = \"raw\"\n",
    "ext = \"parquet\"\n",
    "blob_name_base = f\"full_2010_2015_v_{ver}\"\n",
    "blob_name = f\"{blob_name_base}_{tag}.{ext}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data DF Checkpoint from Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Load data_df from Azure checkpoint\n",
    "load_data_checkpoint = True\n",
    "\n",
    "if load_data_checkpoint:\n",
    "    data_df = None\n",
    "    local_file = tmp_dir + os.sep + blob_name \n",
    "    if not (os.path.exists(local_file)): # <--- when would this ever be true?\n",
    "        azStorageClient = AzStorageClient(az_cred_file)\n",
    "        file_stream = azStorageClient.downloadBlob2Stream(container, blob_name)\n",
    "        data_df = pd.read_parquet(file_stream, engine='pyarrow')\n",
    "        data_df.to_parquet(local_file)\n",
    "    else:\n",
    "        data_df = pd.read_parquet(local_file)\n",
    "\n",
    "    print(f\"Data size: {data_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load Site data\n",
    "site_metadata_df = pd.read_csv(site_metadata_filename, usecols=['site_id', 'filename', 'IGBP'])\n",
    "\n",
    "# only focus on target sites\n",
    "site_metadata_df.dropna(inplace=True)\n",
    "\n",
    "# Group IGBP\n",
    "site_metadata_df['gen_IGBP'] = site_metadata_df['IGBP']\n",
    "site_metadata_df['gen_IGBP'].replace('WSA', 'SAV', inplace=True)\n",
    "site_metadata_df['gen_IGBP'].replace('CSH', 'SHB', inplace=True)\n",
    "site_metadata_df['gen_IGBP'].replace('OSH', 'SHB', inplace=True)\n",
    "site_metadata_df.drop(site_metadata_df[site_metadata_df['gen_IGBP'] == 'WAT'].index, inplace = True)\n",
    "\n",
    "# Get available sites in the datasets\n",
    "available_sites = data_df['site_id'].unique()\n",
    "site_data_df = site_metadata_df.loc[site_metadata_df['site_id'].isin(available_sites)]\n",
    "print(f\"available sites: {site_data_df.shape}\")\n",
    "\n",
    "# Conduct k-fold splitting\n",
    "n = 5\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42) # Add random state for reproducibility\n",
    "folds = skf.split(site_data_df['site_id'], site_data_df['gen_IGBP'])\n",
    "\n",
    "site_splits = []\n",
    "for i, (train_index, test_index) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    data_df = site_data_df[['site_id', 'gen_IGBP']].iloc[test_index]\n",
    "    sites = list(data_df.site_id.unique())\n",
    "    print(f\"  Count={test_index.shape}\")\n",
    "    print(f\"  IGBP ={np.sort(data_df.gen_IGBP.unique())}\")\n",
    "    print(f\"  Sites={sites}\")\n",
    "    print(\"\")\n",
    "\n",
    "    site_splits.append(sites)\n",
    "\n",
    "# print all sites\n",
    "print(site_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format some more here before saving (do we save folds or site pslits?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out as dict\n",
    "joblib.dump({'folds': folds}, os.path.join(preproc_objects_dir, 'folds_dict.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
