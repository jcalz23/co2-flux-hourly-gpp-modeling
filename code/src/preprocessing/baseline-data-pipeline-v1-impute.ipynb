{"cells":[{"cell_type":"markdown","id":"OA1ZkEPP3zLz","metadata":{"id":"OA1ZkEPP3zLz"},"source":["# Notebook Setup"]},{"cell_type":"code","execution_count":16,"id":"ko8n8_z9pkBE","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1676881577346,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"ko8n8_z9pkBE"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  IN_COLLAB = True\n","else:\n","  IN_COLLAB = False\n","\n","#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n","#MY_HOME_ABS_PATH = \"/content/drive/MyDrive/W210/co2-flux-hourly-gpp-modeling\"\n","MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n","\n","if IN_COLLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')"]},{"cell_type":"markdown","id":"WQOpIAzZ32Ek","metadata":{"id":"WQOpIAzZ32Ek"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":17,"id":"be2d4d6a","metadata":{"executionInfo":{"elapsed":6410,"status":"ok","timestamp":1676881747897,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"be2d4d6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install required modules quietly\n","required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n","\n","for p in required_packages: \n","  try:\n","      __import__(p)\n","  except ImportError:\n","      %pip install {p} --quiet\n","\n","import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","os.chdir(MY_HOME_ABS_PATH)\n","import math\n","import json\n","\n","import pandas as pd\n","import numpy as np\n","from calendar import monthrange\n","from datetime import datetime\n","from io import BytesIO\n","from sklearn.impute import KNNImputer\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load locale custome modules\n","import sys\n","if IN_COLLAB:\n","  os.chdir(MY_HOME_ABS_PATH)\n","  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n","else:\n","  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n","\n","from CloudIO.AzStorageClient import AzStorageClient\n","from data_pipeline_lib import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)"]},{"cell_type":"markdown","id":"O4RveeAZ3qm8","metadata":{"id":"O4RveeAZ3qm8"},"source":["# Define Constants"]},{"cell_type":"code","execution_count":18,"id":"8652c8af","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676881773208,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"8652c8af"},"outputs":[],"source":["root_dir =  MY_HOME_ABS_PATH\n","tmp_dir =  root_dir + os.sep + '.tmp'\n","raw_data_dir = tmp_dir\n","data_dir = root_dir + os.sep + 'data'\n","cred_dir = root_dir + os.sep + '.cred'\n","az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n","\n","if IN_COLLAB:\n","  raw_data_dir = \"/content/drive/MyDrive/CO2_flux_gpp_modeling/DS_capstone_23Spring_CO2/Data/half_hourly_data\"\n","\n","site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'\n","monthly_data_filename = data_dir + os.sep + 'monthly-interpolated-v3.csv'\n","\n","# File\n","container = \"baseline-data\"\n","ext = \"parquet\"\n","ver = \"1-i\"\n","blob_name_base = f\"baseline_all_v_{ver}\"\n","train_blob_name_base = f\"baseline-train-v-{ver}\"\n","test_blob_name_base = f\"baseline-test-v-{ver}\""]},{"cell_type":"code","execution_count":19,"id":"VEPrPTsIP-6v","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676881773524,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"VEPrPTsIP-6v"},"outputs":[],"source":["# Define features and target variables of the data pipelines\n","hourly_features = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA',\n","                     'datetime', 'year', 'month', 'day', 'hour', 'date',\n","                     'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', \n","                     'IGBP', 'koppen']\n","target_variable_qc = 'NEE_VUT_REF_QC'\n","target_variable = 'GPP_NT_VUT_REF'\n","metadata_features = ['site_id', 'filename', 'lat', 'long', 'koppen_sub', 'koppen_main',\n","                     'koppen_name', 'c3c4', 'c4_percent', 'monthly_data_available']\n","\n","# Define the features to use in KNN imputer, only using real values as cat are same per site\n","knn_exclude_cols = ['date', 'datetime', 'year', 'month', 'hour', 'day', 'minute', 'site_id', 'IGBP', 'koppen']\n","knn_imp_cols = [x for x in hourly_features + ['GPP_NT_VUT_REF'] if x not in knn_exclude_cols]"]},{"cell_type":"code","execution_count":20,"id":"a14e0595","metadata":{"executionInfo":{"elapsed":320,"status":"ok","timestamp":1676881773523,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"a14e0595"},"outputs":[],"source":["# \"Golden\" Sites\n","tier1_sites = [\"IT-Lav\", \"US-NR1\", \"US-Vcp\"]#, \"FR-Pue\", \"CH-Lae\", \"US-Var\", \"US-Ne2\", \"ES-LJu\", \"US-Ton\"]\n","#tier2_sites = [\"US-UMB\", \"US-Me2\", \"FI-Hyy\", \"US-NR1\", \"IT-Lav\", \"US-Wkg\", \"US-ARM\", \"US-SRM\"]\n","\n","train_sites = tier1_sites# + tier2_sites\n","\n","# Selected Test Sites\n","test_sites = ['IT-Lsn']\n","#test_sites = # [\"US-GLE\", # ENF, Cold\n","              # \"US-AR1\", # GRA, Temperate\n","              # \"US-Seg\", # GRA, Arid\n","            #   \"US-FR2\", # WSA, Temperate\n","            #   \"ES-LM2\", # WSA, Arid\n","            #   \"CA-Cbo\", # DBF, Cold\n","            #   \"FR-Lam\", # CRO, Temperate\n","            #   \"IT-Cpz\", # EBF, Temperate\n","            #   \"CN-Cha\", # MF Cold\n","            #   \"IT-Lsn\", # OSH, Temperate\n","            #   ]"]},{"cell_type":"code","execution_count":24,"id":"fab0c08a","metadata":{},"outputs":[],"source":["# Define imput params\n","impute = True\n","impute_method = 'ffill'\n","impute_global = True # <---- for now\n","resample = True\n","time_col = 'datetime'\n","duration = 'H'\n","\n","# KNNImputer params (if used)\n","k=5\n","weights='uniform'\n","n_fit=20000"]},{"attachments":{},"cell_type":"markdown","id":"df5869b9","metadata":{},"source":["[] no__imp, no_resample\n","[] no__imp, resample\n","[] ffill\n","[] ffill + resample\n","[] ffill + resample + global\n","[] knn + resample\n","[] knn + resample + global"]},{"cell_type":"markdown","id":"bIoy86rH4hRH","metadata":{"id":"bIoy86rH4hRH"},"source":["# Stage 1: Trim and Merge Site Metadata"]},{"cell_type":"code","execution_count":25,"id":"143f9416","metadata":{},"outputs":[],"source":["class PrepareAllSitesHourly:\n","    def __init__(self, site_metadata_filename, monthly_data_filename, train_sites, test_sites, \n","                hourly_features, metadata_features, target_variable_qc, target_variable, data_dir):\n","        self.site_metadata_filename = site_metadata_filename\n","        self.monthly_data_filename = monthly_data_filename\n","        self.train_sites = train_sites\n","        self.test_sites = test_sites\n","        self.hourly_features = hourly_features\n","        self.metadata_features = metadata_features\n","        self.target_variable_qc = target_variable_qc\n","        self.target_variable = target_variable\n","        self.data_dir = data_dir\n","\n","    def add_time_index(self, df, time_col, duration):\n","        df['gap_flag'] = int(0)\n","        df.sort_values(time_col, inplace=True)\n","        df.set_index(time_col, inplace=True)\n","        df = df.resample(duration).first()\n","        df = df.reset_index()\n","        df.index.name='timestep_idx'\n","        df = df.reset_index()\n","        df['gap_flag'].fillna(1, inplace=True)\n","\n","        # Fix time records that are NA for new rows\n","        df['year'] = df['datetime'].dt.year.astype(int)\n","        df['month'] = df['datetime'].dt.month.astype(int)\n","        df['day'] = df['datetime'].dt.day.astype(int)\n","        df['hour'] = df['datetime'].dt.hour.astype(int)\n","        df['date'] = df['datetime'].dt.date\n","\n","        return df\n","\n","\n","    def knn_impute_site(self, site_df, knn_imp_cols, k, weights, n_fit):\n","        # Fit and transform the data using KNNImputer, format as DF\n","        group_knn_df = site_df[knn_imp_cols].copy()\n","        group_knn_df = group_knn_df.dropna(axis=1, how='all') # drop col if all NA, need to globally impute later\n","\n","        # Get subset of rows to speed up impute time (instead of fitting on every single record)\n","        na_mask = group_knn_df.isna().any(axis=1)\n","        na_rows = group_knn_df[na_mask]\n","        not_na_rows = group_knn_df.dropna().sample(n=n_fit)\n","\n","        # Execute imputation\n","        imputer = KNNImputer(n_neighbors=k, weights=weights)\n","        imputer.fit(not_na_rows)\n","        imputed_group = imputer.transform(na_rows)\n","        imputed_group = pd.DataFrame(imputed_group, columns=group_knn_df.columns)\n","\n","        # Reinsert NA rows\n","        group_knn_df.loc[na_mask] = imputed_group\n","\n","        # Fill NA in initial site/group df\n","        site_df.fillna(group_knn_df, inplace=True)\n","\n","        return site_df\n","\n","\n","    def knn_impute_global(self, df, knn_imp_cols, k, weights, n_fit):\n","        print(\"Begin global imputing for fully missing features at site-level\")\n","        print(f\"NA values remaining before global impute: {df.isna().sum().sum()}\")\n","        \n","        # Create copy\n","        df_inds = df.index\n","        data_df_copy = df[knn_imp_cols].copy()\n","        data_df_copy.reset_index(drop=True, inplace=True)\n","\n","        # Use Global Imputing for Sites that have 100% of one feature missing (couldn't impute at site-level)\n","        na_mask = data_df_copy.isna().any(axis=1)\n","        na_inds = na_mask[na_mask==True].index\n","        na_rows = data_df_copy.loc[na_inds, ].copy()\n","        not_na_rows = data_df_copy.dropna().sample(n=n_fit)\n","\n","        # Execute imputation\n","        imputer = KNNImputer(n_neighbors=k, weights=weights)\n","        imputer.fit(not_na_rows)\n","        na_rows_imp = imputer.transform(na_rows)\n","        na_rows_imp = pd.DataFrame(na_rows_imp, columns=na_rows.columns)\n","\n","        # Reinsert NA rows\n","        na_rows_imp.set_index(na_inds, inplace=True)\n","        data_df_copy.loc[na_inds] = na_rows_imp\n","        data_df_copy.set_index(df_inds, inplace=True)\n","\n","        # Fill NA in initial site/group df\n","        df.fillna(data_df_copy, inplace=True)\n","        print(f\"Final NA Count: {df.isna().sum().sum()}\")\n","\n","        return df\n","    \n","\n","    def site_data_cleanup(self, site_metadata_df, knn_imp_cols, resample, impute, impute_method,\n","                         impute_global, k, weights, n_fit, time_col, duration):\n","        data_df = None\n","        qc_flags_features = [s for s in self.hourly_features if \"_QC\" in s]\n","\n","        ## PRINT THE PLAN\n","        if impute:\n","            print(f\"Filling missing values with {impute_method} at site-level, then at global-level at end\")\n","        else:\n","            print(\"Filling all NA values with -1\")\n","\n","        ## SITE-LEVEL CLEANING -> CONCATENATE\n","        for i, r in tqdm(site_metadata_df[['site_id','filename']].iterrows()):        \n","            if not r.filename or type(r.filename) != type(\"\"):\n","                print(f'ERROR: {r.site_id} is missing hourly data.')\n","                continue\n","\n","            # Prepare hourly site df\n","            local_filename = self.data_dir + os.sep + r.filename\n","            site_df = pd.read_csv(local_filename, usecols = [self.target_variable, self.target_variable_qc] + self.hourly_features)\n","\n","            # Format columns\n","            site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n","            site_df['date'] = pd.to_datetime(site_df['date'])\n","            site_df['minute'] = site_df['datetime'].dt.minute\n","            if len(qc_flags_features) != 0:\n","                site_df[qc_flags_features] = site_df[qc_flags_features].astype('int')\n","            site_df['site_id'] = r.site_id\n","\n","\n","            # ----------------- #\n","            # LATER: FILTER SITE-DF TO THE BEST SEQUENCE OF X YEARS (e.g., if we only use 1.5 years per site)\n","            # ----------------- #\n","\n","\n","            # Move from HH to H level\n","            site_df = site_df.loc[site_df['datetime'].dt.minute == 0, ].copy()\n","\n","            # For records with bad target QC, make NAN and impute\n","            site_df.loc[site_df[self.target_variable_qc] == 3, self.target_variable] = np.nan\n","            site_df.drop([self.target_variable_qc], axis=1, inplace=True)\n","\n","            # Resample to add rows for missing timesteps, assign timestep_idx and \"gap_flag\"\n","            if resample:\n","                site_df = self.add_time_index(site_df, time_col, duration)\n","            else:\n","                site_df.sort_values(time_col, inplace=True)\n","                site_df = site_df.reset_index()\n","                site_df.index.name='timestep_idx'\n","                site_df = site_df.reset_index()\n","\n","            # Impute missing values at site-level, otherwise fillna w/ -1 at very end\n","            if impute:\n","                if impute_method=='ffill': # select most recent record\n","                    site_df.sort_values(time_col, ascending=True, inplace=True)\n","                    site_df.fillna(method=\"ffill\", inplace=True)\n","                    \n","                elif impute_method=='knn': # use KNNImputer\n","                    site_df = self.knn_impute_site(site_df, knn_imp_cols, k, weights, n_fit)\n","\n","            # When done cleaning site -> concatenate site_dfs together into global data_df\n","            if type(data_df) == type(None):\n","                data_df = site_df\n","            else:\n","                data_df = pd.concat([data_df, site_df])\n","\n","        ## Handle Global Missing Data (if 100% of feature missing for one site)\n","        if data_df.isna().sum().sum() != 0:\n","            if impute_global:\n","                print(\"Filling global NA with KNNImpute\")\n","                data_df = self.knn_impute_global(data_df, knn_imp_cols, k, weights, n_fit)\n","            elif type(impute_method) != type(None):\n","                data_df.fillna(-1, inplace=True)\n","                print(\"Filling global NA w/ -1\")\n","            elif type(impute_method) == type(None):\n","                print(\"Not filling global NA values\")\n","        print(f\"NA values remaining at end of cleanup: {data_df.isna().sum().sum()}\")\n","\n","        return data_df\n","\n","\n","    def prep_metadata(self):\n","        site_metadata_df = pd.read_csv(self.site_metadata_filename, usecols = self.metadata_features)\n","        site_metadata_df = site_metadata_df.loc[site_metadata_df['site_id'].isin(self.train_sites + self.test_sites), ]\n","        site_metadata_df = site_metadata_df.loc[site_metadata_df['monthly_data_available']=='Yes', ] # <---- not including sites that have zero monthly data (ask team)\n","        site_metadata_df.reset_index(inplace=True, drop=True)\n","        return site_metadata_df\n","\n","\n","    def merge_site_metadata(self, data_df, site_metadata_df):\n","        site_metadata_df = site_metadata_df.drop(['filename', 'koppen_main', 'koppen_name', 'monthly_data_available'], axis=1)\n","        data_df = data_df.merge(site_metadata_df, how='left', left_on='site_id', right_on='site_id')\n","        print(f\"Data size after after merged with site metadata: {data_df.shape}\")\n","        return data_df\n","\n","\n","    def merge_monthly_data(self, data_df):\n","        # Prep monthly\n","        monthly_df = pd.read_csv(self.monthly_data_filename)\n","        monthly_df = monthly_df.loc[monthly_df['SITE_ID'].isin(self.train_sites + self.test_sites)]\n","        monthly_df.reset_index(inplace=True, drop=True)\n","        monthly_df[['year','month', 'MODIS_LC']] = monthly_df[['year','month', 'MODIS_LC']].astype('int')\n","        print(f\"# NAs in monthly: {monthly_df.isna().sum().sum()}\")\n","        print(f\"Num site-year-months (monthly): {len(monthly_df[['SITE_ID', 'year', 'month']].drop_duplicates())}\")\n","\n","        # Merge\n","        data_df = data_df.merge(monthly_df, how='left',\n","                        left_on =['site_id', 'year', 'month'],\n","                        right_on=['SITE_ID', 'year', 'month'])\n","        data_df.drop('SITE_ID', axis=1, inplace=True)\n","        print(f\"Data size after after merged with monthly data: {data_df.shape}\")\n","        return data_df\n","\n","\n","    def all_sites_all_sources(self, knn_imp_cols, resample, impute, impute_method, impute_global, k, weights, n_fit, time_col, duration):\n","        site_metadata_df = self.prep_metadata()\n","        data_df = self.site_data_cleanup(site_metadata_df, knn_imp_cols, resample, impute, impute_method, \n","                                        impute_global, k, weights, n_fit, time_col, duration)\n","\n","        # Merge with site metadata and monthly data\n","        print(f\"NA values remaining at beginning of merges: {data_df.isna().sum().sum()}\")\n","        data_df = self.merge_site_metadata(data_df, site_metadata_df)\n","        data_df = self.merge_monthly_data(data_df)\n","        print(f\"NA values remaining at end of merges: {data_df.isna().sum().sum()}\")\n","\n","        #reorder columns\n","        features = data_df.columns.to_list()\n","        features.remove(target_variable)\n","        data_df = data_df[([target_variable] + features)]\n","        print(f\"Num site-year-months (data_df): {len(data_df[['site_id', 'year', 'month']].drop_duplicates())}\")\n","\n","        return data_df\n"]},{"cell_type":"code","execution_count":26,"id":"5fb2d575","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Filling missing values with ffill at site-level, then at global-level at end\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:02,  1.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Filling global NA with KNNImpute\n","Begin global imputing for fully missing features at site-level\n","NA values remaining before global impute: 146400\n","Final NA Count: 0\n","NA values remaining at end of cleanup: 0\n","NA values remaining at beginning of merges: 0\n","Data size after after merged with site metadata: (435432, 34)\n","# NAs in monthly: 0\n","Num site-year-months (monthly): 596\n","Data size after after merged with monthly data: (435432, 50)\n","NA values remaining at end of merges: 0\n","Num site-year-months (data_df): 596\n"]}],"source":["prep_hourly = PrepareAllSitesHourly(site_metadata_filename, monthly_data_filename, train_sites, test_sites, \n","                                    hourly_features, metadata_features, target_variable_qc, target_variable, raw_data_dir)\n","\n","data_df = prep_hourly.all_sites_all_sources(knn_imp_cols, resample, impute, impute_method, impute_global,\n","                                            k, weights, n_fit, time_col, duration)"]},{"cell_type":"code","execution_count":null,"id":"6531b65c","metadata":{},"outputs":[],"source":["# Weird print statements!"]},{"cell_type":"code","execution_count":null,"id":"76eef796","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"adebb181","metadata":{"id":"adebb181"},"source":["# CheckPoint: Upload Data to Azure Storage Blob as Parquet"]},{"cell_type":"code","execution_count":null,"id":"28f95f13","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5166,"status":"ok","timestamp":1676881861261,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"28f95f13","outputId":"7a176ebe-eaf0-4d8b-bbf9-102a040f2607"},"outputs":[],"source":["# Upload to Azure Storage Blob\n","# ref: https://stackoverflow.com/a/54666079\n","data_cleanup_checkpoint = False\n","tag = \"raw\"\n","blob_name = f\"{blob_name_base}_{tag}.{ext}\"\n","\n","if data_cleanup_checkpoint:\n","  parquet_file = BytesIO()\n","  data_df.to_parquet(parquet_file, engine='pyarrow')\n","  parquet_file.seek(0)\n","\n","  print(f\"Uploading raw data checkpoint to Azure\")\n","  azStorageClient = AzStorageClient(az_cred_file)\n","  azStorageClient.uploadBlob(container, blob_name, parquet_file, overwrite=True)"]},{"cell_type":"code","execution_count":null,"id":"gJAutd_p4HAW","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6592,"status":"ok","timestamp":1676881867840,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"gJAutd_p4HAW","outputId":"b6d2cc89-d35c-4209-8a06-0e528f5321a4"},"outputs":[],"source":["# Save non-transformed data\n","get_non_transform_train_test = False\n","if get_non_transform_train_test:\n","  data_transformer = TFTDataTransformer(train_sites, test_sites,\n","                                        None, data_df)\n","  train_df, test_df = data_transformer.get_test_train_raw()\n","  print(\"Train data peak:\")\n","  display(train_df.head(5))\n","  print(\"Test data peak:\")\n","  display(test_df.head(5))\n","\n","  train_blob_name= f\"{train_blob_name_base}-{tag}.{ext}\"\n","  test_blob_name= f\"{test_blob_name_base}-{tag}.{ext}\"\n","  data_transformer.upload_train_test_to_azure(az_cred_file, container,\\\n","                                              train_blob_name, test_blob_name)"]},{"cell_type":"markdown","id":"wTLUtJLqeZ_Y","metadata":{"id":"wTLUtJLqeZ_Y"},"source":["# Stage 2: Data Transform - Convert to Model Ready Data\n","- Encode data\n","- Split into train and test\n","- Assemble data through VectorAssembler\n","- Normalized data through minmax"]},{"cell_type":"code","execution_count":null,"id":"LsJFFiQpkY4t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1606,"status":"ok","timestamp":1676881986796,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"LsJFFiQpkY4t","outputId":"568b5161-b6ff-47e5-f3ac-3d01713727bc"},"outputs":[],"source":["load_data_from_previous_checkpoint = False\n","useSpark = False\n","\n","raw_data_file_path = None\n","if load_data_from_previous_checkpoint:\n","  data_df = None\n","  raw_data_file_path = tmp_dir + os.sep + blob_name\n","  print(f\"loading {raw_data_file_path}...\")\n","  if not (os.path.exists(raw_data_file_path)):\n","      if not (os.path.exists(tmp_dir)):\n","          os.mkdir(tmp_dir)\n","      azStorageClient = AzStorageClient(az_cred_file)\n","      file_stream = azStorageClient.downloadBlob2Stream(container, blob_name)\n","      data_df = pd.read_parquet(file_stream, engine='pyarrow')\n","      data_df.to_parquet(raw_data_file_path)\n","  \n","if useSpark:\n","  data_transformer = PySparkMLDataTransformer(spark, train_sites, test_sites,\n","                                              raw_data_file_path, data_df)\n","else:\n","  data_transformer = TFTDataTransformer(train_sites, test_sites,\n","                                              raw_data_file_path, data_df)"]},{"cell_type":"code","execution_count":null,"id":"vVG1cfktFRKB","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":19255,"status":"ok","timestamp":1676882006047,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"vVG1cfktFRKB","outputId":"9bc5d0aa-b42e-4f09-87b7-755163c900ef"},"outputs":[],"source":["timestamp_col = ['datetime']\n","target_col = 'GPP_NT_VUT_REF'\n","\n","if useSpark: # Spark ML Data Transformer\n","  categorical_cols = ['IGBP', 'c3c4', 'koppen_sub', 'koppen', 'MODIS_PFT', 'MODIS_LC'] \n","  data_transformer.data_transform(categorical_cols, timestamp_col, target_col)\n","\n","  print(\"Train data peak:\")\n","  data_transformer.train_df.show(5, False)\n","  print(\"Test data peak:\")\n","  data_transformer.test_df.show(5, False)\n","\n","  train_blob_name= f\"{train_blob_name_base}\"\n","  test_blob_name= f\"{test_blob_name_base}\"\n","\n","else: # TFT Data Transformer\n","  categorical_cols = ['IGBP', 'c3c4', 'koppen_sub', 'koppen', 'site_id',\n","                      'year', 'month', 'day', 'hour', 'minute',\n","                      'MODIS_PFT', 'MODIS_LC']\n","  realNum_cols = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA', \n","                  'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7',\n","                  'lat', 'long', 'c4_percent',\n","                  'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN', 'CSIF-SIFdaily',\n","                  'PET', 'Ts', 'ESACCI-sm', 'NDWI', 'Percent_Snow', 'Fpar', 'Lai',\n","                  'LST_Day', 'LST_Night']\n","  backup_cols = ['IGBP', 'koppen','site_id']\n","  data_transformer.data_transform(categorical_cols, realNum_cols, backup_cols,\\\n","                                  timestamp_col, target_col)\n","\n","  print(\"Train data peak:\")\n","  display(data_transformer.train_df.head(5))\n","  print(\"Test data peak:\")\n","  display(data_transformer.test_df.head(5))\n","\n","  train_blob_name= f\"{train_blob_name_base}.{ext}\"\n","  test_blob_name= f\"{test_blob_name_base}.{ext}\""]},{"cell_type":"markdown","id":"ZbLIYew87W7Q","metadata":{"id":"ZbLIYew87W7Q"},"source":["# Checkpoint: Upload train and test to Azure Blob Storage"]},{"cell_type":"code","execution_count":null,"id":"yt0bei5H7enS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3957,"status":"ok","timestamp":1676882009988,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"yt0bei5H7enS","outputId":"18173579-dc0d-44b0-de50-e3d27fd83987"},"outputs":[],"source":["final_checkpoint = True\n","\n","if final_checkpoint:\n","  data_transformer.upload_train_test_to_azure(az_cred_file, container, \\\n","                                            train_blob_name, test_blob_name)"]},{"cell_type":"code","execution_count":null,"id":"fa7b10c5","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fe277de3","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"aa4620a7","metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","id":"31692d0e","metadata":{},"source":["# Fix Class"]},{"cell_type":"code","execution_count":null,"id":"4e8d4f13","metadata":{},"outputs":[],"source":["def add_time_index(df, time_col):\n","    df['gap_flag'] = int(0)\n","    df.sort_values(time_col, inplace=True)\n","    df.set_index(time_col, inplace=True)\n","    df = df.resample(duration).first()\n","    df = df.reset_index()\n","    df.index.name='timestep_idx'\n","    df = df.reset_index()\n","    df['gap_flag'].fillna(1, inplace=True)\n","\n","    # Fix time records that are NA for new rows\n","    df['year'] = df['datetime'].dt.year.astype(int)\n","    df['month'] = df['datetime'].dt.month.astype(int)\n","    df['day'] = df['datetime'].dt.day.astype(int)\n","    df['hour'] = df['datetime'].dt.hour.astype(int)\n","    df['date'] = df['datetime'].dt.date\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"2773f262","metadata":{},"outputs":[],"source":["def knn_impute_site(site_df, imp_cols, k, weights, n):\n","    # Fit and transform the data using KNNImputer, format as DF\n","    group_knn_df = site_df[imp_cols].copy()\n","    group_knn_df = group_knn_df.dropna(axis=1, how='all') # drop col if all NA, need to globally impute later\n","\n","    # Get subset of rows to speed up impute time (instead of fitting on every single record)\n","    na_mask = group_knn_df.isna().any(axis=1)\n","    na_rows = group_knn_df[na_mask]\n","    not_na_rows = group_knn_df.dropna().sample(n=n)\n","\n","    # Execute imputation\n","    imputer = KNNImputer(n_neighbors=k, weights=weights)\n","    imputer.fit(not_na_rows)\n","    imputed_group = imputer.transform(na_rows)\n","    imputed_group = pd.DataFrame(imputed_group, columns=group_knn_df.columns)\n","\n","    # Reinsert NA rows\n","    group_knn_df.loc[na_mask] = imputed_group\n","\n","    # Fill NA in initial site/group df\n","    site_df.fillna(group_knn_df, inplace=True)\n","\n","    return site_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def knn_impute_global(self, df):\n","    print(\"Begin global imputing for fully missing features at site-level\")\n","    print(f\"NA values remaining before global impute: {df.isna().sum().sum()}\")\n","    \n","    # Create copy\n","    df_inds = df.index\n","    data_df_copy = df[self.imp_cols].copy()\n","    data_df_copy.reset_index(drop=True, inplace=True)\n","\n","    # Use Global Imputing for Sites that have 100% of one feature missing (couldn't impute at site-level)\n","    na_mask = data_df_copy.isna().any(axis=1)\n","    na_inds = na_mask[na_mask==True].index\n","    na_rows = data_df_copy.loc[na_inds, ].copy()\n","    not_na_rows = data_df_copy.dropna().sample(n=n)\n","\n","    # Execute imputation\n","    imputer = KNNImputer(n_neighbors=self.k, weights=self.weights)\n","    imputer.fit(not_na_rows)\n","    na_rows_imp = imputer.transform(na_rows)\n","    na_rows_imp = pd.DataFrame(na_rows_imp, columns=na_rows.columns)\n","\n","    # Reinsert NA rows\n","    na_rows_imp.set_index(na_inds, inplace=True)\n","    data_df_copy.loc[na_inds] = na_rows_imp\n","    data_df_copy.set_index(df_inds, inplace=True)\n","\n","    # Fill NA in initial site/group df\n","    df.fillna(data_df_copy, inplace=True)\n","    print(f\"Final NA Count: {df.isna().sum().sum()}\")\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"3948a735","metadata":{},"outputs":[],"source":["def merge_site_metadata(self, data_df, site_metadata):\n","    data_df = data_df.merge(site_metadata_df, how='left', left_on='site_id', right_on='site_id')\n","\n","    return data_df\n","\n","\n","def all_sites_all_sources(self):\n","    data_df = self.site_data_cleanup()\n","\n","    # Merge with site metadata\n","    data_df = self.merge_site_metadata(data_df, self.site_metadata_df.drop(['filename', 'koppen_main', 'koppen_name'], axis=1))\n","    print(f\"Data size after after merged with site metadata: {data_df.shape}\")\n","\n","    # Merge with monthly data\n","    data_df = data_df.merge(monthly_df.drop('date', axis=1), how='left',\n","                            left_on =['site_id', 'year', 'month'],\n","                            right_on=['SITE_ID', 'year', 'month'])\n","    data_df.drop('SITE_ID', axis=1, inplace=True)\n","    print(f\"Data size after after merged with monthly data: {data_df.shape}\")\n","\n","    #reorder columns\n","    features = data_df.columns.to_list()\n","    features.remove(target_variable)\n","    data_df = data_df[([target_variable] + features)]\n","\n","    return data_df\n"]},{"cell_type":"code","execution_count":null,"id":"4bef474d","metadata":{},"outputs":[],"source":["def site_data_cleanup(time_col, imp_cols, k, weights, n):\n","    data_df = None\n","    qc_flags_features = [s for s in self.hourly_features if \"_QC\" in s]\n","\n","    ## PRINT THE PLAN\n","    if self.impute:\n","        print(f\"Filling missing values with {self.impute_method} at site-level, then at global-level at end\")\n","    else:\n","        print(\"Filling all NA values with -1\")\n","\n","    ## SITE-LEVEL CLEANING -> CONCATENATE\n","    for i, r in tqdm(self.site_metadata_df[['site_id','filename']].iterrows()):        \n","        if not r.filename or type(r.filename) != type(\"\"):\n","            print(f'ERROR: {r.site_id} is mssing hourly data.')\n","            continue\n","\n","        # Prepare hourly site df\n","        local_filename = self.data_dir + os.sep + r.filename\n","        site_df = pd.read_csv(local_filename, usecols = [self.target_variable, self.target_variable_qc] + self.hourly_features)\n","\n","        # Format columns\n","        site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n","        site_df['date'] = pd.to_datetime(site_df['date'])\n","        site_df['minute'] = site_df['datetime'].dt.minute\n","        if len(qc_flags_features) != 0:\n","            site_df[qc_flags_features] = site_df[qc_flags_features].astype('int')\n","        site_df['site_id'] = r.site_id\n","\n","\n","        # ----------------- #\n","        # LATER: FILTER SITE-DF TO THE BEST SEQUENCE OF X YEARS (e.g., if we only use 1.5 years per site)\n","        # ----------------- #\n","\n","\n","        # Move from HH to H level\n","        site_df = site_df.loc[site_df['datetime'].dt.minute == 0, ].copy()\n","\n","        # Drop rows with NAs, or bad NEE_VUT_REF_QC for Target Variable <-------------------- MAKE BAD_QC_FLAGS==3 -> NAN to be imputed\n","        #site_df.dropna(subset=[self.target_variable], axis=0, inplace=True)\n","        #site_df.drop(site_df[site_df[self.target_variable_qc] == 3].index, inplace = True)\n","        #site_df.drop([self.target_variable_qc], axis=1, inplace=True)\n","\n","        # Resample to add rows for missing timesteps, assign timestep_idx and \"gap_flag\"\n","        if self.resample:\n","            site_df = self.add_time_index(site_df, time_col)\n","        else:\n","            site_df.sort_values(self.time_col, inplace=True)\n","            site_df = site_df.reset_index()\n","            site_df.index.name='timestep_idx'\n","            site_df = site_df.reset_index()\n","\n","        # Impute missing values at site-level, otherwise fillna w/ -1 at very end\n","        if self.impute:\n","            if self.impute_method=='ffill': # select most recent record\n","                site_df.sort_values(self.time_col, ascending=True, inplace=True)\n","                site_df.fillna(method=\"ffill\", inplace=True)\n","                \n","            elif self.impute_method=='knn': # use KNNImputer\n","                site_df = knn_impute_site(site_df, imp_cols, k, weights, n)\n","\n","        # When done cleaning site -> concatenate site_dfs together into global data_df\n","        if type(data_df) == type(None):\n","            data_df = site_df\n","        else:\n","            data_df = pd.concat([data_df, site_df])\n","\n","    ## Handle Global Data\n","    # If we imputed at site-level already, there may be some features 100% missing for site...\n","    # ... -> thus, we need to impute using global data to fill these\n","    if data_df.isna().sum().sum() != 0:\n","        if self.impute_global:\n","            print(\"Filling global NA with KNNImpute\")\n","            data_df = self.knn_impute_global(data_df, self.imp_cols, self.k, self.weights, self.n)\n","        elif type(self.impute_method) != type(None):\n","            data_df.fillna(-1, inplace=True)\n","            print(\"Filling global NA w/ -1\")\n","        elif type(self.impute_method) == type(None):\n","            print(\"Not filling global NA values\")\n","    print(f\"NA values remaining at end of cleanup: {data_df.isna().sum().sum()}\")\n","\n","    return data_df"]}],"metadata":{"colab":{"collapsed_sections":["OA1ZkEPP3zLz","O4RveeAZ3qm8","UF5VqdIA6Iv1"],"provenance":[]},"kernelspec":{"display_name":"src-rYFmMrY7-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"}}},"nbformat":4,"nbformat_minor":5}
