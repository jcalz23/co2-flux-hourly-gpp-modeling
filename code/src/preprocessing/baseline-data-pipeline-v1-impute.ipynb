{"cells":[{"cell_type":"markdown","id":"OA1ZkEPP3zLz","metadata":{"id":"OA1ZkEPP3zLz"},"source":["# Notebook Setup"]},{"cell_type":"code","execution_count":1,"id":"ko8n8_z9pkBE","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1676881577346,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"ko8n8_z9pkBE"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  IN_COLLAB = True\n","else:\n","  IN_COLLAB = False\n","\n","#TODO: CHANGE THIS BASED ON YOUR OWN LOCAL SETTINGS\n","#MY_HOME_ABS_PATH = \"/content/drive/MyDrive/W210/co2-flux-hourly-gpp-modeling\"\n","MY_HOME_ABS_PATH = \"/Users/jetcalz07/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling\"\n","\n","if IN_COLLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')"]},{"cell_type":"markdown","id":"WQOpIAzZ32Ek","metadata":{"id":"WQOpIAzZ32Ek"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":2,"id":"be2d4d6a","metadata":{"executionInfo":{"elapsed":6410,"status":"ok","timestamp":1676881747897,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"be2d4d6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install required modules quietly\n","required_packages = ['geopandas', 'pyspark', 'azure-storage-blob']\n","\n","for p in required_packages: \n","  try:\n","      __import__(p)\n","  except ImportError:\n","      %pip install {p} --quiet\n","\n","import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","os.chdir(MY_HOME_ABS_PATH) # <------------------ ADDED\n","import math\n","import json\n","\n","import pandas as pd\n","from calendar import monthrange\n","from datetime import datetime\n","from io import BytesIO\n","from sklearn.impute import KNNImputer # <----------- ADDED\n","from tqdm import tqdm # <----------- ADDED\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# Load locale custome modules\n","import sys\n","if IN_COLLAB:\n","  os.chdir(MY_HOME_ABS_PATH)\n","  sys.path.insert(0,os.path.abspath(\"./code/src/tools\"))\n","else:\n","  sys.path.append(os.path.abspath(\"./code/src/tools\"))\n","\n","from CloudIO.AzStorageClient import AzStorageClient\n","from data_pipeline_lib import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)"]},{"cell_type":"markdown","id":"O4RveeAZ3qm8","metadata":{"id":"O4RveeAZ3qm8"},"source":["# Define Constants"]},{"cell_type":"code","execution_count":3,"id":"8652c8af","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676881773208,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"8652c8af"},"outputs":[],"source":["root_dir =  MY_HOME_ABS_PATH\n","tmp_dir =  root_dir + os.sep + '.tmp'\n","raw_data_dir = tmp_dir\n","data_dir = root_dir + os.sep + 'data'\n","cred_dir = root_dir + os.sep + '.cred'\n","az_cred_file = cred_dir + os.sep + 'azblobcred.json'\n","\n","if IN_COLLAB:\n","  raw_data_dir = \"/content/drive/MyDrive/CO2_flux_gpp_modeling/DS_capstone_23Spring_CO2/Data/half_hourly_data\"\n","\n","site_metadata_filename = data_dir + os.sep + 'site-metadata.csv'\n","monthly_data_filename = data_dir + os.sep + 'monthly-interpolated-v3.csv'\n","\n","# File\n","container = \"baseline-data\"\n","ext = \"parquet\"\n","ver = \"1-i\"\n","blob_name_base = f\"baseline_all_v_{ver}\"\n","train_blob_name_base = f\"baseline-train-v-{ver}\"\n","test_blob_name_base = f\"baseline-test-v-{ver}\""]},{"cell_type":"code","execution_count":10,"id":"VEPrPTsIP-6v","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676881773524,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"VEPrPTsIP-6v"},"outputs":[],"source":["# Define features and target variables of the data pipelines\n","included_features = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA',\n","                     'datetime', 'year', 'month', 'day', 'hour', 'date',\n","                     'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', \n","                     'IGBP', 'koppen']\n","target_variable_qc = 'NEE_VUT_REF_QC'\n","target_variable = 'GPP_NT_VUT_REF'\n","metadata_features = ['site_id', 'filename', 'lat', 'long', 'koppen_sub', 'koppen_main',\n","                     'koppen_name', 'c3c4', 'c4_percent', 'monthly_data_available']\n","\n","# Define the features to use in KNN imputer, only using real values as cat are same per site\n","knn_exclude_cols = ['date', 'datetime', 'year', 'month', 'hour', 'day', 'minute', 'site_id', 'IGBP', 'koppen']\n","knn_imp_cols = [x for x in included_features + ['GPP_NT_VUT_REF'] if x not in knn_exclude_cols]"]},{"cell_type":"code","execution_count":11,"id":"a14e0595","metadata":{"executionInfo":{"elapsed":320,"status":"ok","timestamp":1676881773523,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"a14e0595"},"outputs":[],"source":["# \"Golden\" Sites\n","tier1_sites = [\"IT-Lav\", \"US-NR1\", \"US-Vcp\"]#, \"FR-Pue\", \"CH-Lae\", \"US-Var\", \"US-Ne2\", \"ES-LJu\", \"US-Ton\"]\n","#tier2_sites = [\"US-UMB\", \"US-Me2\", \"FI-Hyy\", \"US-NR1\", \"IT-Lav\", \"US-Wkg\", \"US-ARM\", \"US-SRM\"]\n","\n","train_sites = tier1_sites# + tier2_sites\n","\n","# Selected Test Sites\n","test_sites = ['IT-Lsn']\n","#test_sites = # [\"US-GLE\", # ENF, Cold\n","              # \"US-AR1\", # GRA, Temperate\n","              # \"US-Seg\", # GRA, Arid\n","            #   \"US-FR2\", # WSA, Temperate\n","            #   \"ES-LM2\", # WSA, Arid\n","            #   \"CA-Cbo\", # DBF, Cold\n","            #   \"FR-Lam\", # CRO, Temperate\n","            #   \"IT-Cpz\", # EBF, Temperate\n","            #   \"CN-Cha\", # MF Cold\n","            #   \"IT-Lsn\", # OSH, Temperate\n","            #   ]"]},{"cell_type":"code","execution_count":12,"id":"fab0c08a","metadata":{},"outputs":[],"source":["# Define imput params\n","impute = False\n","resample = True\n","impute_method = 'ffill'\n","impute_global = False\n","time_col = 'datetime'\n","duration = 'H'\n","\n","# KNNImputer params (if used)\n","k=5\n","weights='uniform'\n","n_fit=20000"]},{"cell_type":"markdown","id":"2fb241d2","metadata":{"id":"2fb241d2"},"source":["# Get Gold Sample Site Data"]},{"cell_type":"code","execution_count":13,"id":"4e6d698f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1676881773704,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"4e6d698f","outputId":"3b4435e9-2b6e-40dc-b7e8-3bbace1aac92"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site_id</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>koppen_sub</th>\n","      <th>koppen_main</th>\n","      <th>koppen_name</th>\n","      <th>c3c4</th>\n","      <th>c4_percent</th>\n","      <th>filename</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>US-NR1</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>Dfc</td>\n","      <td>C3</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>US-Vcp</td>\n","      <td>35.86240</td>\n","      <td>-106.59740</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>Dfb</td>\n","      <td>C3</td>\n","      <td>0.04000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-Vcp.csv</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>IT-Lav</td>\n","      <td>45.95620</td>\n","      <td>11.28132</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>Dfb</td>\n","      <td>C3</td>\n","      <td>3.57000</td>\n","      <td>data_full_half_hourly_raw_v0_1_IT-Lav.csv</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  site_id      lat       long  koppen_sub  koppen_main koppen_name c3c4  \\\n","0  US-NR1 40.03290 -105.54640          27            4         Dfc   C3   \n","1  US-Vcp 35.86240 -106.59740          26            4         Dfb   C3   \n","2  IT-Lav 45.95620   11.28132          26            4         Dfb   C3   \n","\n","   c4_percent                                   filename  \n","0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv  \n","1     0.04000  data_full_half_hourly_raw_v0_1_US-Vcp.csv  \n","2     3.57000  data_full_half_hourly_raw_v0_1_IT-Lav.csv  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Load site metadata\n","site_metadata_df = pd.read_csv(site_metadata_filename, usecols = metadata_features)\n","\n","# Keep sites that are in train/test AND have monthly data available\n","site_metadata_df = site_metadata_df.loc[site_metadata_df['site_id'].isin(train_sites + test_sites), ]\n","site_metadata_df = site_metadata_df.loc[site_metadata_df['monthly_data_available']=='Yes', ]\n","site_metadata_df.drop(columns='monthly_data_available', inplace=True)\n","site_metadata_df.reset_index(inplace=True, drop=True)\n","site_metadata_df.head(3)"]},{"cell_type":"markdown","id":"UF5VqdIA6Iv1","metadata":{"id":"UF5VqdIA6Iv1"},"source":["# Get Monthly Data"]},{"cell_type":"code","execution_count":14,"id":"ZN4dMFym6H5g","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1428,"status":"ok","timestamp":1676881775125,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"ZN4dMFym6H5g","outputId":"bb8f3124-6d6e-428b-b0b3-28b29e1e00c3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SITE_ID</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>TIMESTAMP</th>\n","      <th>BESS-PAR</th>\n","      <th>BESS-PARdiff</th>\n","      <th>BESS-RSDN</th>\n","      <th>CSIF-SIFdaily</th>\n","      <th>PET</th>\n","      <th>Ts</th>\n","      <th>ESACCI-sm</th>\n","      <th>MODIS_LC</th>\n","      <th>NDWI</th>\n","      <th>Percent_Snow</th>\n","      <th>Fpar</th>\n","      <th>Lai</th>\n","      <th>LST_Day</th>\n","      <th>LST_Night</th>\n","      <th>MODIS_PFT</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>US-NR1</td>\n","      <td>2001</td>\n","      <td>1</td>\n","      <td>200101</td>\n","      <td>37</td>\n","      <td>16</td>\n","      <td>87</td>\n","      <td>0.07419</td>\n","      <td>-0.00427</td>\n","      <td>262.23570</td>\n","      <td>-1.00000</td>\n","      <td>8</td>\n","      <td>0.42081</td>\n","      <td>16.51613</td>\n","      <td>0.53000</td>\n","      <td>0.80000</td>\n","      <td>270.22000</td>\n","      <td>262.84000</td>\n","      <td>SA</td>\n","      <td>2001-01-01</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>US-NR1</td>\n","      <td>2001</td>\n","      <td>2</td>\n","      <td>200102</td>\n","      <td>47</td>\n","      <td>24</td>\n","      <td>110</td>\n","      <td>0.07939</td>\n","      <td>-0.00625</td>\n","      <td>264.60532</td>\n","      <td>-1.00000</td>\n","      <td>8</td>\n","      <td>0.57521</td>\n","      <td>10.28571</td>\n","      <td>0.48000</td>\n","      <td>0.80000</td>\n","      <td>270.56000</td>\n","      <td>262.52000</td>\n","      <td>SA</td>\n","      <td>2001-02-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  SITE_ID  year  month  TIMESTAMP  BESS-PAR  BESS-PARdiff  BESS-RSDN  \\\n","0  US-NR1  2001      1     200101        37            16         87   \n","1  US-NR1  2001      2     200102        47            24        110   \n","\n","   CSIF-SIFdaily      PET        Ts  ESACCI-sm  MODIS_LC    NDWI  \\\n","0        0.07419 -0.00427 262.23570   -1.00000         8 0.42081   \n","1        0.07939 -0.00625 264.60532   -1.00000         8 0.57521   \n","\n","   Percent_Snow    Fpar     Lai   LST_Day  LST_Night MODIS_PFT        date  \n","0      16.51613 0.53000 0.80000 270.22000  262.84000        SA  2001-01-01  \n","1      10.28571 0.48000 0.80000 270.56000  262.52000        SA  2001-02-01  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Load monthly metadata\n","monthly_df = pd.read_csv(monthly_data_filename)\n","monthly_df = monthly_df.loc[monthly_df['SITE_ID'].isin(train_sites + test_sites)]\n","monthly_df.reset_index(inplace=True, drop=True)\n","monthly_df[['year','month', 'MODIS_LC']] = monthly_df[['year','month', 'MODIS_LC']].astype('int')\n","monthly_df.head(2)"]},{"cell_type":"markdown","id":"bIoy86rH4hRH","metadata":{"id":"bIoy86rH4hRH"},"source":["# Stage 1: Trim and Merge Site Metadata"]},{"cell_type":"code","execution_count":21,"id":"143f9416","metadata":{},"outputs":[],"source":["class PrepareAllSitesHourly:\n","    def __init__(self, site_metadata_df, monthly_df, included_features, target_variable_qc,\n","                 target_variable, knn_imp_cols, train_sites, test_sites,\n","                 resample, impute, impute_method, impute_global, k, weights, n_fit, data_dir, time_col, duration):\n","        self.site_metadata_df = site_metadata_df\n","        self.monthly_df = monthly_df\n","        self.included_features = included_features\n","        self.target_variable_qc = target_variable_qc\n","        self.target_variable = target_variable\n","        self.train_sites = train_sites\n","        self.test_sites = test_sites\n","        self.resample = resample\n","        self.impute = impute\n","        self.impute_method = impute_method\n","        self.imp_cols = knn_imp_cols\n","        self.impute_global = impute_global\n","        self.k = k\n","        self.n_fit = n_fit\n","        self.weights = weights\n","        self.data_dir = data_dir\n","        self.time_col = time_col\n","        self.duration = duration\n","\n","\n","    def add_time_index(self, df):\n","        df['gap_flag'] = int(0)\n","        df.sort_values(self.time_col, inplace=True)\n","        df.set_index(self.time_col, inplace=True)\n","        df = df.resample(self.duration).first()\n","        df = df.reset_index()\n","        df.index.name='timestep_idx'\n","        df = df.reset_index()\n","        df['gap_flag'].fillna(1, inplace=True)\n","        #df.loc[df['gap_flag']!= 0, 'gap_flag'] = int(1) # add flag to new records\n","\n","        # Fix time records that are NA for new rows\n","        df['year'] = df['datetime'].dt.year.astype(int)\n","        df['month'] = df['datetime'].dt.month.astype(int)\n","        df['day'] = df['datetime'].dt.day.astype(int)\n","        df['hour'] = df['datetime'].dt.hour.astype(int)\n","        df['date'] = df['datetime'].dt.date\n","\n","        return df\n","\n","\n","    def knn_impute_site(self, site_df):\n","        # Fit and transform the data using KNNImputer, format as DF\n","        group_knn_df = site_df[self.imp_cols].copy()\n","        group_knn_df = group_knn_df.dropna(axis=1, how='all') # drop col if all NA, need to globally impute later\n","\n","        # Get subset of rows to speed up impute time (instead of fitting on every single record)\n","        na_mask = group_knn_df.isna().any(axis=1)\n","        na_rows = group_knn_df[na_mask]\n","        not_na_rows = group_knn_df.dropna().sample(n=self.n)\n","\n","        # Execute imputation\n","        imputer = KNNImputer(n_neighbors=self.k, weights=self.weights)\n","        imputer.fit(not_na_rows)\n","        imputed_group = imputer.transform(na_rows)\n","        imputed_group = pd.DataFrame(imputed_group, columns=group_knn_df.columns)\n","\n","        # Reinsert NA rows\n","        group_knn_df.loc[na_mask] = imputed_group\n","\n","        # Fill NA in initial site/group df\n","        site_df.fillna(group_knn_df, inplace=True)\n","\n","        return site_df\n","\n","\n","    def knn_impute_global(self, df):\n","        print(\"Begin global imputing for fully missing features at site-level\")\n","        print(f\"NA values remaining before global impute: {df.isna().sum().sum()}\")\n","        \n","        # Create copy\n","        df_inds = df.index\n","        data_df_copy = df[self.imp_cols].copy()\n","        data_df_copy.reset_index(drop=True, inplace=True)\n","\n","        # Use Global Imputing for Sites that have 100% of one feature missing (couldn't impute at site-level)\n","        na_mask = data_df_copy.isna().any(axis=1)\n","        na_inds = na_mask[na_mask==True].index\n","        na_rows = data_df_copy.loc[na_inds, ].copy()\n","        not_na_rows = data_df_copy.dropna().sample(n=n)\n","\n","        # Execute imputation\n","        imputer = KNNImputer(n_neighbors=self.k, weights=self.weights)\n","        imputer.fit(not_na_rows)\n","        na_rows_imp = imputer.transform(na_rows)\n","        na_rows_imp = pd.DataFrame(na_rows_imp, columns=na_rows.columns)\n","\n","        # Reinsert NA rows\n","        na_rows_imp.set_index(na_inds, inplace=True)\n","        data_df_copy.loc[na_inds] = na_rows_imp\n","        data_df_copy.set_index(df_inds, inplace=True)\n","\n","        # Fill NA in initial site/group df\n","        df.fillna(data_df_copy, inplace=True)\n","        print(f\"Final NA Count: {df.isna().sum().sum()}\")\n","\n","        return df\n","    \n","\n","    def site_data_cleanup(self):\n","        data_df = None\n","        qc_flags_features = [s for s in self.included_features if \"_QC\" in s]\n","\n","        ## PRINT THE PLAN\n","        if self.impute:\n","            print(f\"Filling missing values with {self.impute_method} at site-level, then at global-level at end\")\n","        else:\n","            print(\"Filling all NA values with -1\")\n","\n","        ## SITE-LEVEL CLEANING -> CONCATENATE\n","        for i, r in tqdm(self.site_metadata_df[['site_id','filename']].iterrows()):        \n","            if not r.filename or type(r.filename) != type(\"\"):\n","                print(f'ERROR: {r.site_id} is mssing hourly data.')\n","                continue\n","\n","            # Prepare hourly site df\n","            local_filename = self.data_dir + os.sep + r.filename\n","            site_df = pd.read_csv(local_filename, usecols = [self.target_variable, self.target_variable_qc] + self.included_features)\n","\n","            # Format columns\n","            site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n","            site_df['date'] = pd.to_datetime(site_df['date'])\n","            site_df['minute'] = site_df['datetime'].dt.minute\n","            if len(qc_flags_features) != 0:\n","                site_df[qc_flags_features] = site_df[qc_flags_features].astype('int')\n","            site_df['site_id'] = r.site_id\n","\n","\n","            # ----------------- #\n","            # LATER: FILTER SITE-DF TO THE BEST SEQUENCE OF X YEARS (e.g., if we only use 1.5 years per site)\n","            # ----------------- #\n","\n","\n","            # Move from HH to H level\n","            site_df = site_df.loc[site_df['datetime'].dt.minute == 0, ].copy()\n","\n","            # Drop rows with NAs, or bad NEE_VUT_REF_QC for Target Variable <-------------------- MAKE BAD_QC_FLAGS==3 -> NAN to be imputed\n","            #site_df.dropna(subset=[self.target_variable], axis=0, inplace=True)\n","            #site_df.drop(site_df[site_df[self.target_variable_qc] == 3].index, inplace = True)\n","            #site_df.drop([self.target_variable_qc], axis=1, inplace=True)\n","\n","            # Resample to add rows for missing timesteps, assign timestep_idx and \"gap_flag\"\n","            if self.resample:\n","                site_df = self.add_time_index(site_df)\n","            else:\n","                site_df.sort_values(self.time_col, inplace=True)\n","                site_df = site_df.reset_index()\n","                site_df.index.name='timestep_idx'\n","                site_df = site_df.reset_index()\n","\n","            # Impute missing values at site-level, otherwise fillna w/ -1 at very end\n","            if self.impute:\n","                if self.impute_method=='ffill': # select most recent record\n","                    site_df.sort_values(self.time_col, ascending=True, inplace=True)\n","                    site_df.fillna(method=\"ffill\", inplace=True)\n","                    \n","                elif self.impute_method=='knn': # use KNNImputer\n","                    site_df = self.knn_impute_site(site_df, self.imp_cols, self.k, self.weights, self.n)\n","\n","            # When done cleaning site -> concatenate site_dfs together into global data_df\n","            if type(data_df) == type(None):\n","                data_df = site_df\n","            else:\n","                data_df = pd.concat([data_df, site_df])\n","\n","        ## Handle Global Data\n","        # If we imputed at site-level already, there may be some features 100% missing for site...\n","        # ... -> thus, we need to impute using global data to fill these\n","        if data_df.isna().sum().sum() != 0:\n","            if self.impute_global:\n","                print(\"Filling global NA with KNNImpute\")\n","                data_df = self.knn_impute_global(data_df, self.imp_cols, self.k, self.weights, self.n)\n","            elif type(self.impute_method) != type(None):\n","                data_df.fillna(-1, inplace=True)\n","                print(\"Filling global NA w/ -1\")\n","            elif type(self.impute_method) == type(None):\n","                print(\"Not filling global NA values\")\n","        print(f\"NA values remaining at end of cleanup: {data_df.isna().sum().sum()}\")\n","\n","        return data_df\n","\n","\n","    def merge_site_metadata(self, data_df, site_metadata):\n","        data_df = data_df.merge(site_metadata_df, how='left', left_on='site_id', right_on='site_id')\n","\n","        return data_df\n","\n","\n","    def all_sites_all_sources(self):\n","        data_df = self.site_data_cleanup()\n","\n","        # Merge with site metadata\n","        data_df = self.merge_site_metadata(data_df, self.site_metadata_df.drop(['filename', 'koppen_main', 'koppen_name'], axis=1))\n","        print(f\"Data size after after merged with site metadata: {data_df.shape}\")\n","\n","        # Merge with monthly data\n","        data_df = data_df.merge(monthly_df.drop('date', axis=1), how='left',\n","                                left_on =['site_id', 'year', 'month'],\n","                                right_on=['SITE_ID', 'year', 'month'])\n","        data_df.drop('SITE_ID', axis=1, inplace=True)\n","        print(f\"Data size after after merged with monthly data: {data_df.shape}\")\n","\n","        #reorder columns\n","        features = data_df.columns.to_list()\n","        features.remove(target_variable)\n","        data_df = data_df[([target_variable] + features)]\n","\n","        return data_df\n"]},{"cell_type":"code","execution_count":22,"id":"02293a34","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Filling all NA values with -1\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:02,  1.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["NA values remaining at end of cleanup: 0\n","Data size after after merged with site metadata: (435432, 37)\n","Data size after after merged with monthly data: (435432, 53)\n"]}],"source":["prep_hourly = PrepareAllSitesHourly(site_metadata_df, monthly_df, included_features, target_variable_qc,\n","                 target_variable, knn_imp_cols, train_sites, test_sites, resample, impute, impute_method, \n","                 impute_global, k, weights, n_fit, raw_data_dir, time_col, duration)\n","data_df = prep_hourly.all_sites_all_sources()"]},{"attachments":{},"cell_type":"markdown","id":"a083dc8e","metadata":{},"source":["## Dev Resampling"]},{"cell_type":"code","execution_count":23,"id":"d5ae1055","metadata":{},"outputs":[],"source":["# Choose sample site\n","site_id = 'US-NR1'\n","filename = f'data_full_half_hourly_raw_v0_1_{site_id}.csv'\n","\n","# Prepare hourly site df\n","local_filename = raw_data_dir + os.sep + filename\n","site_df = pd.read_csv(local_filename, usecols = [target_variable, target_variable_qc] + included_features)\n","site_df['site_id'] = site_id\n","\n","# Move from HH to H level\n","site_df['datetime'] = pd.to_datetime(site_df['datetime'])\n","site_df = site_df.loc[site_df['datetime'].dt.minute == 0, ].copy()\n"]},{"cell_type":"code","execution_count":24,"id":"0a7fdf12","metadata":{},"outputs":[],"source":["# Resampling\n","def add_time_index(df_init, time_col, duration):\n","    df_init['gap_flag'] = int(0)\n","    df = df_init.sort_values(by=[time_col])\n","    df.set_index(time_col, inplace=True)\n","    df = df.resample(duration).first() # <-- changed to first to preserve categorical features\n","    df = df.reset_index()\n","    df.index.name='timestep_idx'\n","    df = df.reset_index()\n","    df.loc[df['gap_flag']!= 0, 'gap_flag'] = int(1) # add flag to new records\n","\n","    # Fix time records that are NA for new rows\n","    df['year'] = df['datetime'].dt.year.astype(int)\n","    df['month'] = df['datetime'].dt.month.astype(int)\n","    df['day'] = df['datetime'].dt.day.astype(int)\n","    df['hour'] = df['datetime'].dt.hour.astype(int)\n","    df['date'] = df['datetime'].dt.date\n","    \n","    return df\n","\n","\n","def impute_ffill(df, time_col, method):\n","    if method == 'ffill':\n","        df.sort_values(time_col, ascending=True, inplace=True)\n","        df.fillna(method=\"ffill\", inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":29,"id":"32d7ef55","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial len: (135384, 28)\n","New len: (137352, 29)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timestep_idx</th>\n","      <th>datetime</th>\n","      <th>TA_ERA</th>\n","      <th>SW_IN_ERA</th>\n","      <th>LW_IN_ERA</th>\n","      <th>VPD_ERA</th>\n","      <th>P_ERA</th>\n","      <th>PA_ERA</th>\n","      <th>NEE_VUT_REF_QC</th>\n","      <th>GPP_NT_VUT_REF</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>hour</th>\n","      <th>date</th>\n","      <th>EVI</th>\n","      <th>NDVI</th>\n","      <th>NIRv</th>\n","      <th>b1</th>\n","      <th>b2</th>\n","      <th>b3</th>\n","      <th>b4</th>\n","      <th>b5</th>\n","      <th>b6</th>\n","      <th>b7</th>\n","      <th>IGBP</th>\n","      <th>koppen</th>\n","      <th>site_id</th>\n","      <th>gap_flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1896</th>\n","      <td>1896</td>\n","      <td>1999-07-20 00:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1897</th>\n","      <td>1897</td>\n","      <td>1999-07-20 01:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>1</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1898</th>\n","      <td>1898</td>\n","      <td>1999-07-20 02:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>2</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1899</th>\n","      <td>1899</td>\n","      <td>1999-07-20 03:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1900</th>\n","      <td>1900</td>\n","      <td>1999-07-20 04:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>4</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>1.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      timestep_idx            datetime  TA_ERA  SW_IN_ERA  LW_IN_ERA  VPD_ERA  \\\n","1896          1896 1999-07-20 00:00:00     NaN        NaN        NaN      NaN   \n","1897          1897 1999-07-20 01:00:00     NaN        NaN        NaN      NaN   \n","1898          1898 1999-07-20 02:00:00     NaN        NaN        NaN      NaN   \n","1899          1899 1999-07-20 03:00:00     NaN        NaN        NaN      NaN   \n","1900          1900 1999-07-20 04:00:00     NaN        NaN        NaN      NaN   \n","\n","      P_ERA  PA_ERA  NEE_VUT_REF_QC  GPP_NT_VUT_REF  year  month  day  hour  \\\n","1896    NaN     NaN             NaN             NaN  1999      7   20     0   \n","1897    NaN     NaN             NaN             NaN  1999      7   20     1   \n","1898    NaN     NaN             NaN             NaN  1999      7   20     2   \n","1899    NaN     NaN             NaN             NaN  1999      7   20     3   \n","1900    NaN     NaN             NaN             NaN  1999      7   20     4   \n","\n","            date  EVI  NDVI  NIRv  b1  b2  b3  b4  b5  b6  b7  IGBP koppen  \\\n","1896  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  None   None   \n","1897  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  None   None   \n","1898  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  None   None   \n","1899  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  None   None   \n","1900  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  None   None   \n","\n","     site_id  gap_flag  \n","1896    None   1.00000  \n","1897    None   1.00000  \n","1898    None   1.00000  \n","1899    None   1.00000  \n","1900    None   1.00000  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timestep_idx</th>\n","      <th>datetime</th>\n","      <th>TA_ERA</th>\n","      <th>SW_IN_ERA</th>\n","      <th>LW_IN_ERA</th>\n","      <th>VPD_ERA</th>\n","      <th>P_ERA</th>\n","      <th>PA_ERA</th>\n","      <th>NEE_VUT_REF_QC</th>\n","      <th>GPP_NT_VUT_REF</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>hour</th>\n","      <th>date</th>\n","      <th>EVI</th>\n","      <th>NDVI</th>\n","      <th>NIRv</th>\n","      <th>b1</th>\n","      <th>b2</th>\n","      <th>b3</th>\n","      <th>b4</th>\n","      <th>b5</th>\n","      <th>b6</th>\n","      <th>b7</th>\n","      <th>IGBP</th>\n","      <th>koppen</th>\n","      <th>site_id</th>\n","      <th>gap_flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1896</th>\n","      <td>1896</td>\n","      <td>1999-07-20 00:00:00</td>\n","      <td>6.55900</td>\n","      <td>0.00000</td>\n","      <td>286.49000</td>\n","      <td>2.15400</td>\n","      <td>0.00000</td>\n","      <td>71.05800</td>\n","      <td>1.00000</td>\n","      <td>0.34003</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ENF</td>\n","      <td>Cold</td>\n","      <td>US-NR1</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1897</th>\n","      <td>1897</td>\n","      <td>1999-07-20 01:00:00</td>\n","      <td>6.55900</td>\n","      <td>0.00000</td>\n","      <td>286.49000</td>\n","      <td>2.15400</td>\n","      <td>0.00000</td>\n","      <td>71.05800</td>\n","      <td>1.00000</td>\n","      <td>0.34003</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>1</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ENF</td>\n","      <td>Cold</td>\n","      <td>US-NR1</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1898</th>\n","      <td>1898</td>\n","      <td>1999-07-20 02:00:00</td>\n","      <td>6.55900</td>\n","      <td>0.00000</td>\n","      <td>286.49000</td>\n","      <td>2.15400</td>\n","      <td>0.00000</td>\n","      <td>71.05800</td>\n","      <td>1.00000</td>\n","      <td>0.34003</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>2</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ENF</td>\n","      <td>Cold</td>\n","      <td>US-NR1</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1899</th>\n","      <td>1899</td>\n","      <td>1999-07-20 03:00:00</td>\n","      <td>6.55900</td>\n","      <td>0.00000</td>\n","      <td>286.49000</td>\n","      <td>2.15400</td>\n","      <td>0.00000</td>\n","      <td>71.05800</td>\n","      <td>1.00000</td>\n","      <td>0.34003</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>3</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ENF</td>\n","      <td>Cold</td>\n","      <td>US-NR1</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1900</th>\n","      <td>1900</td>\n","      <td>1999-07-20 04:00:00</td>\n","      <td>6.55900</td>\n","      <td>0.00000</td>\n","      <td>286.49000</td>\n","      <td>2.15400</td>\n","      <td>0.00000</td>\n","      <td>71.05800</td>\n","      <td>1.00000</td>\n","      <td>0.34003</td>\n","      <td>1999</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>4</td>\n","      <td>1999-07-20</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ENF</td>\n","      <td>Cold</td>\n","      <td>US-NR1</td>\n","      <td>1.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      timestep_idx            datetime  TA_ERA  SW_IN_ERA  LW_IN_ERA  VPD_ERA  \\\n","1896          1896 1999-07-20 00:00:00 6.55900    0.00000  286.49000  2.15400   \n","1897          1897 1999-07-20 01:00:00 6.55900    0.00000  286.49000  2.15400   \n","1898          1898 1999-07-20 02:00:00 6.55900    0.00000  286.49000  2.15400   \n","1899          1899 1999-07-20 03:00:00 6.55900    0.00000  286.49000  2.15400   \n","1900          1900 1999-07-20 04:00:00 6.55900    0.00000  286.49000  2.15400   \n","\n","       P_ERA   PA_ERA  NEE_VUT_REF_QC  GPP_NT_VUT_REF  year  month  day  hour  \\\n","1896 0.00000 71.05800         1.00000         0.34003  1999      7   20     0   \n","1897 0.00000 71.05800         1.00000         0.34003  1999      7   20     1   \n","1898 0.00000 71.05800         1.00000         0.34003  1999      7   20     2   \n","1899 0.00000 71.05800         1.00000         0.34003  1999      7   20     3   \n","1900 0.00000 71.05800         1.00000         0.34003  1999      7   20     4   \n","\n","            date  EVI  NDVI  NIRv  b1  b2  b3  b4  b5  b6  b7 IGBP koppen  \\\n","1896  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  ENF   Cold   \n","1897  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  ENF   Cold   \n","1898  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  ENF   Cold   \n","1899  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  ENF   Cold   \n","1900  1999-07-20  NaN   NaN   NaN NaN NaN NaN NaN NaN NaN NaN  ENF   Cold   \n","\n","     site_id  gap_flag  \n","1896  US-NR1   1.00000  \n","1897  US-NR1   1.00000  \n","1898  US-NR1   1.00000  \n","1899  US-NR1   1.00000  \n","1900  US-NR1   1.00000  "]},"metadata":{},"output_type":"display_data"}],"source":["print(f\"Initial len: {site_df.shape}\")\n","resampled_df = add_time_index(site_df, 'datetime', 'H')\n","print(f\"New len: {resampled_df.shape}\")\n","display(resampled_df.loc[resampled_df['gap_flag']!= 0, ].head())\n","\n","\n","df_ffill = impute_ffill(resampled_df.copy(), 'datetime', 'ffill')\n","print(\"\\n\")\n","display(df_ffill.loc[df_ffill['gap_flag']!= 0, ].head())"]},{"cell_type":"code","execution_count":null,"id":"ae76789e","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0c3dd7c1","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"599a1cca","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"adebb181","metadata":{"id":"adebb181"},"source":["# CheckPoint: Upload Data to Azure Storage Blob as Parquet"]},{"cell_type":"code","execution_count":11,"id":"28f95f13","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5166,"status":"ok","timestamp":1676881861261,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"28f95f13","outputId":"7a176ebe-eaf0-4d8b-bbf9-102a040f2607"},"outputs":[],"source":["# Upload to Azure Storage Blob\n","# ref: https://stackoverflow.com/a/54666079\n","data_cleanup_checkpoint = False\n","tag = \"raw\"\n","blob_name = f\"{blob_name_base}_{tag}.{ext}\"\n","\n","if data_cleanup_checkpoint:\n","  parquet_file = BytesIO()\n","  data_df.to_parquet(parquet_file, engine='pyarrow')\n","  parquet_file.seek(0)\n","\n","  print(f\"Uploading raw data checkpoint to Azure\")\n","  azStorageClient = AzStorageClient(az_cred_file)\n","  azStorageClient.uploadBlob(container, blob_name, parquet_file, overwrite=True)"]},{"cell_type":"code","execution_count":12,"id":"gJAutd_p4HAW","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6592,"status":"ok","timestamp":1676881867840,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"gJAutd_p4HAW","outputId":"b6d2cc89-d35c-4209-8a06-0e528f5321a4"},"outputs":[],"source":["# Save non-transformed data\n","get_non_transform_train_test = False\n","if get_non_transform_train_test:\n","  data_transformer = TFTDataTransformer(train_sites, test_sites,\n","                                        None, data_df)\n","  train_df, test_df = data_transformer.get_test_train_raw()\n","  print(\"Train data peak:\")\n","  display(train_df.head(5))\n","  print(\"Test data peak:\")\n","  display(test_df.head(5))\n","\n","  train_blob_name= f\"{train_blob_name_base}-{tag}.{ext}\"\n","  test_blob_name= f\"{test_blob_name_base}-{tag}.{ext}\"\n","  data_transformer.upload_train_test_to_azure(az_cred_file, container,\\\n","                                              train_blob_name, test_blob_name)"]},{"cell_type":"markdown","id":"wTLUtJLqeZ_Y","metadata":{"id":"wTLUtJLqeZ_Y"},"source":["# Stage 2: Data Transform - Convert to Model Ready Data\n","- Encode data\n","- Split into train and test\n","- Assemble data through VectorAssembler\n","- Normalized data through minmax"]},{"cell_type":"code","execution_count":13,"id":"LsJFFiQpkY4t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1606,"status":"ok","timestamp":1676881986796,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"LsJFFiQpkY4t","outputId":"568b5161-b6ff-47e5-f3ac-3d01713727bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data size: (411854, 50).\n"]}],"source":["load_data_from_previous_checkpoint = False\n","useSpark = False\n","\n","raw_data_file_path = None\n","if load_data_from_previous_checkpoint:\n","  data_df = None\n","  raw_data_file_path = tmp_dir + os.sep + blob_name\n","  print(f\"loading {raw_data_file_path}...\")\n","  if not (os.path.exists(raw_data_file_path)):\n","      if not (os.path.exists(tmp_dir)):\n","          os.mkdir(tmp_dir)\n","      azStorageClient = AzStorageClient(az_cred_file)\n","      file_stream = azStorageClient.downloadBlob2Stream(container, blob_name)\n","      data_df = pd.read_parquet(file_stream, engine='pyarrow')\n","      data_df.to_parquet(raw_data_file_path)\n","  \n","if useSpark:\n","  data_transformer = PySparkMLDataTransformer(spark, train_sites, test_sites,\n","                                              raw_data_file_path, data_df)\n","else:\n","  data_transformer = TFTDataTransformer(train_sites, test_sites,\n","                                              raw_data_file_path, data_df)"]},{"cell_type":"code","execution_count":14,"id":"vVG1cfktFRKB","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":19255,"status":"ok","timestamp":1676882006047,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"vVG1cfktFRKB","outputId":"9bc5d0aa-b42e-4f09-87b7-755163c900ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data size: (411854, 52).\n","Data size after encoding: (411854, 52)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>GPP_NT_VUT_REF</th>\n","      <th>TA_ERA</th>\n","      <th>SW_IN_ERA</th>\n","      <th>LW_IN_ERA</th>\n","      <th>VPD_ERA</th>\n","      <th>P_ERA</th>\n","      <th>PA_ERA</th>\n","      <th>datetime</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>hour</th>\n","      <th>EVI</th>\n","      <th>NDVI</th>\n","      <th>NIRv</th>\n","      <th>b1</th>\n","      <th>b2</th>\n","      <th>b3</th>\n","      <th>b4</th>\n","      <th>b5</th>\n","      <th>b6</th>\n","      <th>b7</th>\n","      <th>IGBP</th>\n","      <th>koppen</th>\n","      <th>minute</th>\n","      <th>site_id</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>koppen_sub</th>\n","      <th>koppen_main</th>\n","      <th>koppen_name</th>\n","      <th>c3c4</th>\n","      <th>c4_percent</th>\n","      <th>filename</th>\n","      <th>TIMESTAMP</th>\n","      <th>BESS-PAR</th>\n","      <th>BESS-PARdiff</th>\n","      <th>BESS-RSDN</th>\n","      <th>CSIF-SIFdaily</th>\n","      <th>PET</th>\n","      <th>Ts</th>\n","      <th>ESACCI-sm</th>\n","      <th>MODIS_LC</th>\n","      <th>NDWI</th>\n","      <th>Percent_Snow</th>\n","      <th>Fpar</th>\n","      <th>Lai</th>\n","      <th>LST_Day</th>\n","      <th>LST_Night</th>\n","      <th>MODIS_PFT</th>\n","      <th>IGBP_name</th>\n","      <th>site_id_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.38329</td>\n","      <td>-1.29800</td>\n","      <td>0.00000</td>\n","      <td>292.59200</td>\n","      <td>1.99800</td>\n","      <td>0.06100</td>\n","      <td>69.38400</td>\n","      <td>1999-05-02 00:00:00</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Cold</td>\n","      <td>0</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>ENF</td>\n","      <td>US-NR1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.41899</td>\n","      <td>-1.54800</td>\n","      <td>0.00000</td>\n","      <td>292.59200</td>\n","      <td>2.01000</td>\n","      <td>0.06100</td>\n","      <td>69.33100</td>\n","      <td>1999-05-02 01:00:00</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Cold</td>\n","      <td>0</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>ENF</td>\n","      <td>US-NR1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.51696</td>\n","      <td>-1.79800</td>\n","      <td>0.00000</td>\n","      <td>282.54700</td>\n","      <td>2.02200</td>\n","      <td>0.00000</td>\n","      <td>69.27800</td>\n","      <td>1999-05-02 02:00:00</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Cold</td>\n","      <td>0</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>ENF</td>\n","      <td>US-NR1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.56481</td>\n","      <td>-1.86100</td>\n","      <td>0.00000</td>\n","      <td>282.54700</td>\n","      <td>2.02300</td>\n","      <td>0.62100</td>\n","      <td>69.27300</td>\n","      <td>1999-05-02 03:00:00</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Cold</td>\n","      <td>0</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>ENF</td>\n","      <td>US-NR1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.58099</td>\n","      <td>-1.92400</td>\n","      <td>0.00000</td>\n","      <td>282.54700</td>\n","      <td>2.02400</td>\n","      <td>0.62100</td>\n","      <td>69.26700</td>\n","      <td>1999-05-02 04:00:00</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>-1.00000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>40.03290</td>\n","      <td>-105.54640</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>Cold</td>\n","      <td>0</td>\n","      <td>0.35000</td>\n","      <td>data_full_half_hourly_raw_v0_1_US-NR1.csv</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>ENF</td>\n","      <td>US-NR1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   GPP_NT_VUT_REF   TA_ERA  SW_IN_ERA  LW_IN_ERA  VPD_ERA   P_ERA   PA_ERA  \\\n","0         0.38329 -1.29800    0.00000  292.59200  1.99800 0.06100 69.38400   \n","1         0.41899 -1.54800    0.00000  292.59200  2.01000 0.06100 69.33100   \n","2         0.51696 -1.79800    0.00000  282.54700  2.02200 0.00000 69.27800   \n","3         0.56481 -1.86100    0.00000  282.54700  2.02300 0.62100 69.27300   \n","4         0.58099 -1.92400    0.00000  282.54700  2.02400 0.62100 69.26700   \n","\n","             datetime  year  month  day  hour      EVI     NDVI     NIRv  \\\n","0 1999-05-02 00:00:00     0      4    1     0 -1.00000 -1.00000 -1.00000   \n","1 1999-05-02 01:00:00     0      4    1     1 -1.00000 -1.00000 -1.00000   \n","2 1999-05-02 02:00:00     0      4    1     2 -1.00000 -1.00000 -1.00000   \n","3 1999-05-02 03:00:00     0      4    1     3 -1.00000 -1.00000 -1.00000   \n","4 1999-05-02 04:00:00     0      4    1     4 -1.00000 -1.00000 -1.00000   \n","\n","        b1       b2       b3       b4       b5       b6       b7  IGBP  \\\n","0 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000     0   \n","1 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000     0   \n","2 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000     0   \n","3 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000     0   \n","4 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000 -1.00000     0   \n","\n","   koppen  minute  site_id      lat       long  koppen_sub  koppen_main  \\\n","0       0       0        2 40.03290 -105.54640           2            4   \n","1       0       0        2 40.03290 -105.54640           2            4   \n","2       0       0        2 40.03290 -105.54640           2            4   \n","3       0       0        2 40.03290 -105.54640           2            4   \n","4       0       0        2 40.03290 -105.54640           2            4   \n","\n","  koppen_name  c3c4  c4_percent                                   filename  \\\n","0        Cold     0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv   \n","1        Cold     0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv   \n","2        Cold     0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv   \n","3        Cold     0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv   \n","4        Cold     0     0.35000  data_full_half_hourly_raw_v0_1_US-NR1.csv   \n","\n","   TIMESTAMP  BESS-PAR  BESS-PARdiff  BESS-RSDN  CSIF-SIFdaily  PET  Ts  \\\n","0        NaN       NaN           NaN        NaN            NaN  NaN NaN   \n","1        NaN       NaN           NaN        NaN            NaN  NaN NaN   \n","2        NaN       NaN           NaN        NaN            NaN  NaN NaN   \n","3        NaN       NaN           NaN        NaN            NaN  NaN NaN   \n","4        NaN       NaN           NaN        NaN            NaN  NaN NaN   \n","\n","   ESACCI-sm  MODIS_LC  NDWI  Percent_Snow  Fpar  Lai  LST_Day  LST_Night  \\\n","0        NaN         3   NaN           NaN   NaN  NaN      NaN        NaN   \n","1        NaN         3   NaN           NaN   NaN  NaN      NaN        NaN   \n","2        NaN         3   NaN           NaN   NaN  NaN      NaN        NaN   \n","3        NaN         3   NaN           NaN   NaN  NaN      NaN        NaN   \n","4        NaN         3   NaN           NaN   NaN  NaN      NaN        NaN   \n","\n","   MODIS_PFT IGBP_name site_id_name  \n","0          3       ENF       US-NR1  \n","1          3       ENF       US-NR1  \n","2          3       ENF       US-NR1  \n","3          3       ENF       US-NR1  \n","4          3       ENF       US-NR1  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Features(47): ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA', 'year', 'month', 'day', 'hour', 'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'IGBP', 'koppen', 'minute', 'site_id', 'lat', 'long', 'koppen_sub', 'koppen_main', 'c3c4', 'c4_percent', 'filename', 'TIMESTAMP', 'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN', 'CSIF-SIFdaily', 'PET', 'Ts', 'ESACCI-sm', 'MODIS_LC', 'NDWI', 'Percent_Snow', 'Fpar', 'Lai', 'LST_Day', 'LST_Night', 'MODIS_PFT']\n","Unique sites in df: <bound method Series.unique of 0         2\n","1         2\n","2         2\n","3         2\n","4         2\n","         ..\n","411849    1\n","411850    1\n","411851    1\n","411852    1\n","411853    1\n","Name: site_id, Length: 411854, dtype: int64>\n","Passed train: ['IT-Lav', 'US-NR1', 'US-Vcp']\n","Passed test: ['IT-Lsn']\n","Train data size: (0, 52).\n","Test data size: (0, 52).\n","Normalizinf features (35): ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA', 'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'lat', 'long', 'koppen_main', 'c4_percent', 'filename', 'TIMESTAMP', 'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN', 'CSIF-SIFdaily', 'PET', 'Ts', 'ESACCI-sm', 'NDWI', 'Percent_Snow', 'Fpar', 'Lai', 'LST_Day', 'LST_Night']\n"]},{"ename":"ValueError","evalue":"Found array with 0 sample(s) (shape=(0, 35)) while a minimum of 1 is required by StandardScaler.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m realNum_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mTA_ERA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSW_IN_ERA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLW_IN_ERA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVPD_ERA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mP_ERA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPA_ERA\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     21\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mEVI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNDVI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNIRv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb7\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mc4_percent\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mBESS-PAR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBESS-PARdiff\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBESS-RSDN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCSIF-SIFdaily\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mPET\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mESACCI-sm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNDWI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPercent_Snow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFpar\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLai\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mLST_Day\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLST_Night\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m backup_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mIGBP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkoppen\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39msite_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m data_transformer\u001b[39m.\u001b[39;49mdata_transform(categorical_cols, realNum_cols, backup_cols,\\\n\u001b[1;32m     28\u001b[0m                                 timestamp_col, target_col)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain data peak:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m display(data_transformer\u001b[39m.\u001b[39mtrain_df\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m))\n","File \u001b[0;32m~/Desktop/MIDS/W210_Capstone/co2-flux-hourly-gpp-modeling/code/src/tools/data_pipeline_lib.py:251\u001b[0m, in \u001b[0;36mTFTDataTransformer.data_transform\u001b[0;34m(self, categorical_cols, realNum_cols, backup_cols, timestamp_cols, target_col)\u001b[0m\n\u001b[1;32m    248\u001b[0m   features\u001b[39m.\u001b[39mremove(f)\n\u001b[1;32m    249\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNormalizinf features (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(features)\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\u001b[39m.\u001b[39;49mfit(train_df[features])\n\u001b[1;32m    252\u001b[0m train_df\u001b[39m.\u001b[39mloc[:,features] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(train_df[features])\n\u001b[1;32m    253\u001b[0m test_df\u001b[39m.\u001b[39mloc[:,features] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(test_df[features])\n","File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/src-rYFmMrY7-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:809\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 809\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n","File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/src-rYFmMrY7-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:844\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \n\u001b[1;32m    814\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    843\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 844\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    845\u001b[0m     X,\n\u001b[1;32m    846\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    847\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    848\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    849\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    850\u001b[0m )\n\u001b[1;32m    851\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    853\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/src-rYFmMrY7-py3.10/lib/python3.10/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n","File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/src-rYFmMrY7-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:909\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 909\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    916\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n","\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 35)) while a minimum of 1 is required by StandardScaler."]}],"source":["timestamp_col = ['datetime']\n","target_col = 'GPP_NT_VUT_REF'\n","\n","if useSpark: # Spark ML Data Transformer\n","  categorical_cols = ['IGBP', 'c3c4', 'koppen_sub', 'koppen', 'MODIS_PFT', 'MODIS_LC'] \n","  data_transformer.data_transform(categorical_cols, timestamp_col, target_col)\n","\n","  print(\"Train data peak:\")\n","  data_transformer.train_df.show(5, False)\n","  print(\"Test data peak:\")\n","  data_transformer.test_df.show(5, False)\n","\n","  train_blob_name= f\"{train_blob_name_base}\"\n","  test_blob_name= f\"{test_blob_name_base}\"\n","\n","else: # TFT Data Transformer\n","  categorical_cols = ['IGBP', 'c3c4', 'koppen_sub', 'koppen', 'site_id',\n","                      'year', 'month', 'day', 'hour', 'minute',\n","                      'MODIS_PFT', 'MODIS_LC']\n","  realNum_cols = ['TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA', 'PA_ERA', \n","                  'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7',\n","                  'lat', 'long', 'c4_percent',\n","                  'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN', 'CSIF-SIFdaily',\n","                  'PET', 'Ts', 'ESACCI-sm', 'NDWI', 'Percent_Snow', 'Fpar', 'Lai',\n","                  'LST_Day', 'LST_Night']\n","  backup_cols = ['IGBP', 'koppen','site_id']\n","  data_transformer.data_transform(categorical_cols, realNum_cols, backup_cols,\\\n","                                  timestamp_col, target_col)\n","\n","  print(\"Train data peak:\")\n","  display(data_transformer.train_df.head(5))\n","  print(\"Test data peak:\")\n","  display(data_transformer.test_df.head(5))\n","\n","  train_blob_name= f\"{train_blob_name_base}.{ext}\"\n","  test_blob_name= f\"{test_blob_name_base}.{ext}\""]},{"cell_type":"markdown","id":"ZbLIYew87W7Q","metadata":{"id":"ZbLIYew87W7Q"},"source":["# Checkpoint: Upload train and test to Azure Blob Storage"]},{"cell_type":"code","execution_count":null,"id":"yt0bei5H7enS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3957,"status":"ok","timestamp":1676882009988,"user":{"displayName":"Mary Chau","userId":"14027067063095206122"},"user_tz":480},"id":"yt0bei5H7enS","outputId":"18173579-dc0d-44b0-de50-e3d27fd83987"},"outputs":[],"source":["final_checkpoint = True\n","\n","if final_checkpoint:\n","  data_transformer.upload_train_test_to_azure(az_cred_file, container, \\\n","                                            train_blob_name, test_blob_name)"]}],"metadata":{"colab":{"collapsed_sections":["OA1ZkEPP3zLz","O4RveeAZ3qm8","UF5VqdIA6Iv1"],"provenance":[]},"kernelspec":{"display_name":"src-rYFmMrY7-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"5cbe69e04c91e7625dfb8f223669796fe243b4d7c88cd4431379e3b6898fe927"}}},"nbformat":4,"nbformat_minor":5}
