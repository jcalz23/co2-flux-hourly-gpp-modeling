2023-04-02 20:48:20.406874: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2023-04-02 20:48:20.409346: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.2023-04-02 20:48:20.456724: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.2023-04-02 20:48:20.457335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2023-04-02 20:48:21.380940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRTGlobal seed set to 42Global seed set to 42GPU available: True (cuda), used: TrueTPU available: False, using: 0 TPU coresIPU available: False, using: 0 IPUsHPU available: False, using: 0 HPUs[rank: 0] Global seed set to 42Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/42023-04-02 20:48:52.948448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT2023-04-02 20:48:53.193982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT2023-04-02 20:48:53.336430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[rank: 3] Global seed set to 42[rank: 3] Global seed set to 42[rank: 1] Global seed set to 42[rank: 1] Global seed set to 42[rank: 2] Global seed set to 42[rank: 2] Global seed set to 42[rank: 3] Global seed set to 42Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4[rank: 2] Global seed set to 42Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4[rank: 1] Global seed set to 42Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4----------------------------------------------------------------------------------------------------distributed_backend=ncclAll distributed processes registered. Starting with 4 processes----------------------------------------------------------------------------------------------------LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]   | Name                               | Type                            | Params----------------------------------------------------------------------------------------0  | loss                               | QuantileLoss                    | 0     1  | logging_metrics                    | ModuleList                      | 0     2  | input_embeddings                   | MultiEmbedding                  | 403   3  | prescalers                         | ModuleDict                      | 1.0 K 4  | static_variable_selection          | VariableSelectionNetwork        | 868   5  | encoder_variable_selection         | VariableSelectionNetwork        | 211 K 6  | decoder_variable_selection         | VariableSelectionNetwork        | 192 K 7  | static_context_variable_selection  | GatedResidualNetwork            | 74.8 K8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 74.8 K9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 74.8 K10 | static_context_enrichment          | GatedResidualNetwork            | 74.8 K11 | lstm_encoder                       | LSTM                            | 149 K 12 | lstm_decoder                       | LSTM                            | 149 K 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 37.3 K14 | post_lstm_add_norm_encoder         | AddNorm                         | 272   15 | static_enrichment                  | GatedResidualNetwork            | 93.3 K16 | multihead_attn                     | InterpretableMultiHeadAttention | 74.4 K17 | post_attn_gate_norm                | GateAddNorm                     | 37.5 K18 | pos_wise_ff                        | GatedResidualNetwork            | 74.8 K19 | pre_output_gate_norm               | GateAddNorm                     | 37.5 K20 | output_layer                       | Linear                          | 959   ----------------------------------------------------------------------------------------1.4 M     Trainable params0         Non-trainable params1.4 M     Total params5.432     Total estimated model params size (MB)Data size: (4862712, 52)Data Columns: Index(['GPP_NT_VUT_REF', 'site_id', 'timestep_idx_local',       'timestep_idx_global', 'datetime', 'date', 'year', 'month', 'day',       'hour', 'TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA',       'PA_ERA', 'EVI', 'NDVI', 'NIRv', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6',       'b7', 'IGBP', 'lat', 'long', 'koppen_sub', 'koppen_main', 'c3c4',       'c4_percent', 'BESS-PAR', 'BESS-PARdiff', 'BESS-RSDN', 'CSIF-SIFdaily',       'PET', 'Ts', 'ESACCI-sm', 'MODIS_LC', 'NDWI', 'Percent_Snow', 'Fpar',       'Lai', 'LST_Day', 'LST_Night', 'MODIS_IGBP', 'MODIS_PFT',       'gap_flag_hour', 'gap_flag_month', 'rfr_pred_gpp'],      dtype='object')NA count: 0Training timestemp length = 8760.Experiment logs saved to /root/co2-flux-hourly-gpp-modeling/data/models/treeft_1YrTrain_1WkEncode_RFR_slim_230402_2048.Subest length: 8760 timesteps for each sitesSubset num train timesteps: 683280Subset num val timesteps: 227760Number of parameters in network: 1357.9kSanity Checking: 0it [00:00, ?it/s]Data size: (4862712, 52)Sanity Checking:   0%|                                                                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|                                                                                                                                                                                                      | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                               | 1/2 [00:00<00:00,  1.49it/s]Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]                                                                                                                                                                                                                                                               Training: 0it [00:00, ?it/s]Training:   0%|                                                                                                                                                                                                                       | 0/1745 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                                                                                                                                                        | 0/1745 [00:00<?, ?it/s][W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())Epoch 0:   0%|                                                                                                                                                                                                              | 1/1745 [00:08<4:02:43,  8.35s/it]Training time: 5421.995967682Best model path: /root/co2-flux-hourly-gpp-modeling/data/models/treeft_1YrTrain_1WkEncode_RFR_slim_230402_2049/version_0/checkpoints/epoch=6-step=9156.ckptSaved model to /root/co2-flux-hourly-gpp-modeling/data/models/treeft_1YrTrain_1WkEncode_RFR_slim_230402_2049/model.pth